# -*- coding: utf-8 -*-
"""1.9 Word2Vec CON Gensim - Python

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18JNNFLDRILCNOfyDCcxyIQqdYLWFZWdV

***Word2Vec***
<br>
**Adri√°n Yared Armas de la Nuez**

Tutorial video:
https://www.youtube.com/watch?v=Z1VsHYcNXDI

# Word2Vec model trainning

Gensim install (gensim is a  library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities, using modern statistical machine learning )
"""

pip install gensim

"""## Imports"""

from gensim.models import Word2Vec,keyedvectors # Does topic modeling and document similarity
import pandas as pd # Does data manipulation and analysis
import nltk # Its a natural Language Processing toolkit.
import kagglehub # Kaggle download library

"""## Import dataset

Downloads a dataset of news titles from Reddit using kagglehub and loads it into a pandas DataFrame.
"""

# Download latest version
path = kagglehub.dataset_download("rootuser/worldnews-on-reddit")
csvPath = path + "/reddit_worldnews_start_to_2016-11-22.csv"
print("Path to dataset files:", path)

"""### Show first 10 results"""

df = pd.read_csv(csvPath)
df.head(10)

"""### Get title values"""

# Get all title values
newsTitles = df["title"].values
newsTitles

"""## Prepare the data

### install punkt
"""

#only once
nltk.download('punkt_tab')

"""### Tokenize word

tokenizes the titles into individual words
"""

newsVec = [nltk.word_tokenize(title) for title in newsTitles]

# Show all vec
newsVec

"""## Train the Word2Vec model"""

# Trains a Word2Vec model on the tokenized news titles. min_count=1 means even words that appear
# only once are considered, and vector_size=32 sets the dimensionality of the word vectors.
model = Word2Vec(newsVec, min_count=1, vector_size=32)

"""## Explore the model"""

# Find similar vectorized words to 'man'
model.wv.most_similar('man')

# Finds words most similar to "man", performs vector arithmetic (king - man + woman),
# and finds the most similar word to the resulting vector (likely "queen").
vec = model.wv['king'] - model.wv['man'] + model.wv['woman']

# This output is a list of tuples, where each tuple represents a word and its similarity
# score to the target vector (vec, which was calculated as king - man + woman).
model.wv.most_similar([vec])

# This output is a list of tuples, where each tuple represents a word and its similarity
# score to the target vector (man).
model.wv['man']

"""# Using pre-trained word2vec model"""

# Gdown downloads files from Google Drive.
pip install gdown

"""## Model download"""

#Download the pre-trained model
!gdown 0B7XkCwpI5KDYNlNUTTlSS21pQmM -O GoogleNews-vectors-negative300.bin.gz
# Unzip
!gzip -d /content/GoogleNews-vectors-negative300.bin.gz

"""## Imports"""

from gensim.models import Word2Vec,keyedvectors # Does topic modeling and document similarity
import pandas as pd # Does data manipulation and analysis
import nltk # Its a natural Language Processing toolkit.

"""## Load the pre-trained model:"""

# Loads a pre-trained Word2Vec model into a variable named model. model = GoogleNews-vectors-negative300
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=100000)

"""## Explore the pre-trained model"""

# print all vec
model["man"]

# Finds words most similar to "man", performs vector arithmetic (king - man + woman),
# and finds the most similar word to the resulting vector (likely "queen").
vec = model ["king"] - model["man"] + model["woman"]

# This output is a list of tuples, where each tuple represents a word and its similarity
# score to the target vector.
model.most_similar([vec])

# The output of model.most_similar([vec]) would likely be a list of words (countries) where "France" is at or near the top, along with possibly other
# countries that have a similar relationship to their capitals as the Germany-Berlin relationship. This is because the vector arithmetic captures the
# "capital city of" relationship
vec = model ["Germany"] - model["Berlin"] + model["Paris"]
model.most_similar([vec])
# This output would indicate that "France" is the most similar word to the vector you calculated,
# followed by "Paris," "Belgium," and so on. The numbers represent the similarity scores.

# The code uses vector arithmetic to find the analogy between Messi and football and applies it to tennis to find a similar entity. The output suggests that the model successfully identifies famous
# tennis players like Nadal and Federer, showcasing how word embeddings can capture relationships and analogies between words and concepts. I hope this helps! Let me know if you have any other questions.
vec = model ["Messi"] - model["football"] + model["tennis"]
model.most_similar([vec])