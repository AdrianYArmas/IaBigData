{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6nCdT0aYCWSYgmLR/lwLY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Implementación Práctica: Transformer desde Cero\n","\n","En este cuaderno, implementaremos los componentes clave de un modelo Transformer siguiendo la arquitectura original \"Attention Is All You Need\". Ejecuta cada celda de código para definir las clases y funciones necesarias."],"metadata":{"id":"p26sEPFpBiiC"}},{"cell_type":"markdown","source":["### Importaciones y Clases Auxiliares\n","Importamos las librerías necesarias y definimos algunas clases fundamentales como la Normalización de Capa (LayerNorm) y la Conexión Residual (SublayerConnection)."],"metadata":{"id":"X6n_V0iJB6NQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZghDd6bBTDU","executionInfo":{"status":"ok","timestamp":1746555042373,"user_tz":-60,"elapsed":12816,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"858f23db-49b7-43aa-d0c1-4fae081a7330"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clases LayerNorm y SublayerConnection definidas.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import math\n","import copy # Para clonar módulos\n","\n","class LayerNorm(nn.Module):\n","    # Construye una capa de LayerNorm.\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        # Parámetros aprendibles gamma (a_2) y beta (b_2)\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps # Pequeño valor para evitar división por cero\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, features)\n","        # Calcula la media y desviación estándar sobre la última dimensión (features)\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        # Aplica la normalización\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","\n","class SublayerConnection(nn.Module):\n","    # Conexión residual seguida de LayerNorm. Nota: La normalización va ANTES de la subcapa aquí.\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        # Aplica la conexión residual a cualquier subcapa del mismo tamaño.\n","        # Normaliza x, pasa por la subcapa (con dropout), y añade la entrada original x (conexión residual)\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","print(\"Clases LayerNorm y SublayerConnection definidas.\")\n"]},{"cell_type":"markdown","source":["### Atención Multi-Cabeza (Multi-Head Attention)"],"metadata":{"id":"QfHxkUKkCOLu"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    # Implementación de la atención multi-cabeza.\n","    def __init__(self, h, d_model, dropout=0.1):\n","        # Toma el número de cabezas (h) y la dimensión del modelo (d_model).\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % h == 0\n","        # Asumimos que d_v siempre es igual a d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        # Creamos 4 capas lineales: para Q, K, V y la salida final\n","        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n","        self.attn = None # Para almacenar los pesos de atención para visualización/análisis\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def attention(self, query, key, value, mask=None, dropout=None):\n","        # Calcula la salida de la atención escalada por producto punto.\n","        d_k = query.size(-1)\n","        # 1. Calcula scores: (Batch, h, Seq_q, d_k) @ (Batch, h, d_k, Seq_k) -> (Batch, h, Seq_q, Seq_k)\n","        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n","        # 2. Aplica máscara (si existe)\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9) # Rellena con valor muy negativo donde mask es 0\n","        # 3. Aplica softmax para obtener pesos\n","        p_attn = torch.softmax(scores, dim=-1)\n","        if dropout is not None:\n","            p_attn = dropout(p_attn)\n","        # 4. Multiplica pesos por Values: (Batch, h, Seq_q, Seq_k) @ (Batch, h, Seq_k, d_k) -> (Batch, h, Seq_q, d_k)\n","        return torch.matmul(p_attn, value), p_attn\n","\n","    def forward(self, query, key, value, mask=None):\n","        # Implementa la figura 2 del paper.\n","        if mask is not None:\n","            # Aplica la misma máscara a todas las cabezas\n","            mask = mask.unsqueeze(1) # (Batch, 1, Seq_q, Seq_k) o (Batch, 1, 1, Seq_k)\n","        nbatches = query.size(0)\n","\n","        # 1) Hacer proyección lineal en batch de d_model => h x d_k\n","        # query, key, value: (Batch, Seq, d_model)\n","        # -> aplica linear y view -> (Batch, Seq, h, d_k)\n","        # -> transpose -> (Batch, h, Seq, d_k)\n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","\n","        # 2) Aplicar atención en todos los vectores proyectados en batch.\n","        # x: (Batch, h, Seq_q, d_k)\n","        # self.attn: (Batch, h, Seq_q, Seq_k)\n","        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n","\n","        # 3) \"Concatenar\" usando view y aplicar proyección lineal final.\n","        # x: (Batch, h, Seq_q, d_k) -> transpose -> (Batch, Seq_q, h, d_k)\n","        # -> contiguous + view -> (Batch, Seq_q, d_model)\n","        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n","        # Aplica la capa lineal final W_o\n","        return self.linears[-1](x)\n","\n","print(\"Clase MultiHeadAttention definida.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKiGcverCR0e","executionInfo":{"status":"ok","timestamp":1746555293853,"user_tz":-60,"elapsed":4,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"bc42ed6d-66d3-401b-e73a-9d1c845334bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase MultiHeadAttention definida.\n"]}]},{"cell_type":"markdown","source":["###Red Feed-Forward (Position-wise)\n","Cada capa del Transformer también contiene una red feed-forward simple que se aplica independientemente a cada posición."],"metadata":{"id":"Zs3omJzkCZLv"}},{"cell_type":"code","source":["class PositionwiseFeedForward(nn.Module):\n","    # Implementa una red FFN.\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff) # Capa de expansión\n","        self.w_2 = nn.Linear(d_ff, d_model) # Capa de contracción\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU() # Activación ReLU (común)\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq, d_model)\n","        # -> w_1 -> (Batch, Seq, d_ff)\n","        # -> activation -> (Batch, Seq, d_ff)\n","        # -> dropout -> (Batch, Seq, d_ff)\n","        # -> w_2 -> (Batch, Seq, d_model)\n","        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n","\n","print(\"Clase PositionwiseFeedForward definida.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOeJy7bOCdkt","executionInfo":{"status":"ok","timestamp":1746555690489,"user_tz":-60,"elapsed":15,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"f988d588-2c1c-4893-b43e-812bacc75a90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase PositionwiseFeedForward definida.\n"]}]},{"cell_type":"markdown","source":["### Embeddings y Codificación Posicional\n","Convertimos los tokens de entrada en vectores (embeddings) y añadimos información sobre su posición en la secuencia."],"metadata":{"id":"kRPXdDajCiC1"}},{"cell_type":"code","source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        # vocab: tamaño del vocabulario.\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model) # Capa de embedding\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq) - Índices de tokens\n","        # -> lut -> (Batch, Seq, d_model)\n","        # Multiplica por sqrt(d_model) según el paper\n","        return self.lut(x) * math.sqrt(self.d_model)\n","\n","class PositionalEncoding(nn.Module):\n","    # Implementa la codificación posicional PE.\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        # max_len: longitud máxima de secuencia esperada.\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Calcula los valores de codificación posicional una vez en log space\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n","        # Término de división para las frecuencias\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        # Calcula seno para posiciones pares y coseno para impares\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0) # Añadir dimensión de batch: (1, max_len, d_model)\n","        # Registrar 'pe' como buffer, no como parámetro entrenable\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq, d_model)\n","        # Añade la codificación posicional a los embeddings\n","        # Necesitamos tomar solo las primeras 'Seq' codificaciones posicionales\n","        # self.pe[:, :x.size(1)] -> (1, Seq, d_model)\n","        x = x + self.pe[:, :x.size(1)].requires_grad_(False) # No requiere gradiente\n","        return self.dropout(x)\n","\n","print(\"Clases Embeddings y PositionalEncoding definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2r41Qp_CmFN","executionInfo":{"status":"ok","timestamp":1746555695877,"user_tz":-60,"elapsed":6,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"e60a69bb-bcf4-414a-cbcd-e4c82aee0f11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases Embeddings y PositionalEncoding definidas.\n"]}]},{"cell_type":"markdown","source":["### Capas del Codificador y Decodificador\n","Ahora ensamblamos las subcapas (atención y feed-forward) para formar las capas del codificador y decodificador."],"metadata":{"id":"p4u0zFLXC6Gk"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    # El Encoder se compone de auto-atención y feed-forward.\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        # size: d_model.\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn # Subcapa de auto-atención\n","        self.feed_forward = feed_forward # Subcapa feed-forward\n","        # Dos conexiones residuales + LayerNorm\n","        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(2)])\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        # Sigue la conexión definida en SublayerConnection.\n","        # 1. Auto-atención (Query, Key, Value son iguales a x)\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        # 2. Feed-forward\n","        return self.sublayer[1](x, self.feed_forward)\n","\n","class DecoderLayer(nn.Module):\n","    # El Decoder se compone de auto-atención, atención fuente-objetivo y feed-forward.\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn # Auto-atención (enmascarada) sobre la salida del decoder\n","        self.src_attn = src_attn   # Atención sobre la salida del encoder (memoria)\n","        self.feed_forward = feed_forward\n","        # Tres conexiones residuales + LayerNorm\n","        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(3)])\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        # x: entrada del decoder (salida previa + pos encoding)\n","        # memory: salida del encoder\n","        # src_mask: máscara para el padding en la secuencia de entrada (encoder)\n","        # tgt_mask: máscara para el padding y para evitar atención futura en la secuencia de salida (decoder)\n","\n","        m = memory\n","        # 1. Auto-atención enmascarada (Query, Key, Value son iguales a x)\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        # 2. Atención sobre la salida del encoder (Query=x, Key=memory, Value=memory)\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        # 3. Capa FeedForward\n","        return self.sublayer[2](x, self.feed_forward)\n","\n","print(\"Clases EncoderLayer y DecoderLayer definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROOkyi-BC_WV","executionInfo":{"status":"ok","timestamp":1746555913371,"user_tz":-60,"elapsed":52,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"95934d4e-59d8-42a6-9ac9-79f397fe6fac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases EncoderLayer y DecoderLayer definidas.\n"]}]},{"cell_type":"markdown","source":["### Ensamblaje del Codificador y Decodificador\n","Apilamos múltiples capas para formar el codificador y el decodificador completos."],"metadata":{"id":"819Lxo1tDB_V"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    # Encoder central con N capas.\n","    def __init__(self, layer, N):\n","        # layer: una instancia de EncoderLayer. N: número de capas.\n","        super(Encoder, self).__init__()\n","        # Clona la capa N veces\n","        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n","        # Normalización final después de la última capa\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, mask):\n","        # Pasa la entrada (y máscara) a través de cada capa.\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","class Decoder(nn.Module):\n","    # Decoder genérico con N capas.\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        # Pasa la entrada (y máscaras) a través de cada capa.\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)\n","\n","print(\"Clases Encoder y Decoder definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jzx_vCvDLyO","executionInfo":{"status":"ok","timestamp":1746555928156,"user_tz":-60,"elapsed":48,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"20e8426e-d332-47f6-ae1c-b2dfbe4ab0b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases Encoder y Decoder definidas.\n"]}]},{"cell_type":"markdown","source":["### Modelo Transformer Completo y Generador\n","Finalmente, unimos el codificador, el decodificador y las capas de embedding, junto con una capa final (Generador) para producir las probabilidades de salida sobre el vocabulario."],"metadata":{"id":"lKL-np4_Db18"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","    # Define la capa de generación lineal estándar + softmax.\n","    def __init__(self, d_model, vocab):\n","        # vocab: tamaño del vocabulario de salida.\n","        super(Generator, self).__init__()\n","        # Proyecta de d_model a la dimensión del vocabulario\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq, d_model)\n","        # -> proj -> (Batch, Seq, vocab)\n","        # Aplicar log_softmax es común para usar con NLLLoss durante el entrenamiento\n","        return torch.log_softmax(self.proj(x), dim=-1)\n","\n","class Transformer(nn.Module):\n","    # Implementación del modelo Transformer completo.\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed # Embedding + Positional Encoding para la entrada\n","        self.tgt_embed = tgt_embed # Embedding + Positional Encoding para la salida\n","        self.generator = generator # Capa lineal final + Softmax\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        # Procesa secuencias de entrada y salida enmascaradas.\n","        # 1. Pasa la entrada por el codificador\n","        memory = self.encode(src, src_mask)\n","        # 2. Pasa la salida del codificador y la entrada del decodificador por el decodificador\n","        return self.decode(memory, src_mask, tgt, tgt_mask)\n","\n","    def encode(self, src, src_mask):\n","        # Aplica embedding + pos encoding a la entrada, luego pasa por el codificador\n","        return self.encoder(self.src_embed(src), src_mask)\n","\n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        # Aplica embedding + pos encoding a la entrada del decodificador, luego pasa por el decodificador\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n","\n","print(\"Clases Generator y Transformer definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmQp4ww6DhxM","executionInfo":{"status":"ok","timestamp":1746556075468,"user_tz":-60,"elapsed":18,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"136fa756-9601-4ee1-efbb-adcb0a03c75e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases Generator y Transformer definidas.\n"]}]},{"cell_type":"markdown","source":["###Función Constructora y Ejemplo de Uso\n","Creamos una función para construir el modelo con hiperparámetros específicos y probamos pasar datos ficticios a través de él."],"metadata":{"id":"isAEHzWHDjz5"}},{"cell_type":"code","source":["def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    # Construye un modelo Transformer completo con hiperparámetros.\n","    c = copy.deepcopy\n","    attn = MultiHeadAttention(h, d_model, dropout)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","\n","    # Crear el modelo completo\n","    model = Transformer(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab)\n","    )\n","\n","    # Inicializar parámetros con Xavier Glorot (importante para convergencia)\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return model\n","\n","# --- Ejemplo de uso --- #\n","\n","# Parámetros de ejemplo (puedes cambiarlos)\n","SRC_VOCAB_SIZE = 1000 # Tamaño vocabulario fuente pequeño para ejemplo\n","TGT_VOCAB_SIZE = 1200 # Tamaño vocabulario objetivo pequeño para ejemplo\n","N_LAYERS = 2        # Número de capas (reducido para rapidez)\n","D_MODEL = 128       # Dimensión del modelo (reducido)\n","D_FF = 256          # Dimensión FeedForward (reducido)\n","NUM_HEADS = 4         # Número de cabezas (reducido)\n","DROPOUT = 0.1\n","\n","# Crear modelo\n","model = make_model(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, N=N_LAYERS, d_model=D_MODEL, d_ff=D_FF, h=NUM_HEADS, dropout=DROPOUT)\n","print(f\"Modelo Transformer creado con {N_LAYERS} capas, d_model={D_MODEL}, {NUM_HEADS} cabezas.\")\n","# print(model) # Descomenta para ver la estructura detallada\n","\n","# Ejemplo de datos dummy (Batch=2, Seq_len_src=10, Seq_len_tgt=8)\n","# Tokens de entrada (valores entre 1 y SRC_VOCAB_SIZE-1, 0 es padding)\n","src_dummy = torch.randint(1, SRC_VOCAB_SIZE, (2, 10))\n","src_dummy[0, 7:] = 0 # Añadir padding a la primera secuencia\n","# Tokens de salida esperados (para entrenamiento teacher forcing)\n","# Valores entre 1 y TGT_VOCAB_SIZE-1, 0 es padding\n","tgt_dummy = torch.randint(1, TGT_VOCAB_SIZE, (2, 8))\n","tgt_dummy[1, 6:] = 0 # Añadir padding a la segunda secuencia\n","# La entrada real al decoder durante el entrenamiento es tgt excepto el último token\n","tgt_dummy_input = tgt_dummy[:, :-1]\n","\n","# Crear máscaras\n","# Máscara de padding para src: (Batch, 1, Seq_len) - True donde NO es padding\n","src_mask_dummy = (src_dummy != 0).unsqueeze(1)\n","# Máscara de padding para tgt_input: (Batch, 1, Seq_len)\n","_tgt_mask_padding = (tgt_dummy_input != 0).unsqueeze(1)\n","# Máscara futura para tgt_input (triangular inferior): (1, Seq_len, Seq_len)\n","def subsequent_mask(size):\n","    \"Enmascara posiciones futuras.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n","    return subsequent_mask == 0 # Devuelve True donde NO está enmascarado\n","\n","_tgt_mask_future = subsequent_mask(tgt_dummy_input.size(-1))\n","# Combinar máscara de padding y futura para el decoder\n","tgt_mask_dummy = _tgt_mask_padding & _tgt_mask_future\n","\n","# Pasar datos por el modelo (solo forward pass, sin entrenamiento)\n","# Asegúrate de que el modelo esté en modo evaluación si no estás entrenando\n","model.eval()\n","with torch.no_grad(): # No calcular gradientes para este ejemplo\n","    output = model(src_dummy, tgt_dummy_input, src_mask_dummy, tgt_mask_dummy)\n","    # Pasar la salida por el generador para obtener log probabilidades\n","    final_output_log_probs = model.generator(output)\n","\n","print(f\"\\nForma de la entrada src: {src_dummy.shape}\")\n","print(f\"Forma de la entrada tgt (input decoder): {tgt_dummy_input.shape}\")\n","print(f\"Forma de la máscara src: {src_mask_dummy.shape}\")\n","print(f\"Forma de la máscara tgt: {tgt_mask_dummy.shape}\")\n","print(f\"Forma de la salida del decoder (antes del generador): {output.shape}\") # Esperado: (Batch, Seq_len_tgt-1, d_model)\n","print(f\"Forma de la salida final (log probs): {final_output_log_probs.shape}\") # Esperado: (Batch, Seq_len_tgt-1, tgt_vocab_size)\n","\n","# Puedes inspeccionar los valores si quieres\n","# print(\"\\nLog probabilidades del primer token de la primera secuencia:\")\n","# print(final_output_log_probs[0, 0, :])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Po32iDHWDwNK","executionInfo":{"status":"ok","timestamp":1746556132423,"user_tz":-60,"elapsed":49,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"da0009f2-8258-4dfb-8386-54ed2a3e46bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo Transformer creado con 2 capas, d_model=128, 4 cabezas.\n","\n","Forma de la entrada src: torch.Size([2, 10])\n","Forma de la entrada tgt (input decoder): torch.Size([2, 7])\n","Forma de la máscara src: torch.Size([2, 1, 10])\n","Forma de la máscara tgt: torch.Size([2, 7, 7])\n","Forma de la salida del decoder (antes del generador): torch.Size([2, 7, 128])\n","Forma de la salida final (log probs): torch.Size([2, 7, 1200])\n"]}]},{"cell_type":"markdown","source":["**Explicación Práctica:**\n","\n","*   Hemos definido todas las clases necesarias para un Transformer.\n","*   La función `make_model` facilita la creación del modelo con diferentes hiperparámetros.\n","*   El ejemplo de uso muestra cómo crear el modelo, preparar datos ficticios (incluyendo padding) y las máscaras correspondientes (padding y futura).\n","*   Finalmente, pasamos los datos por el modelo para obtener las log-probabilidades de salida. En un escenario real, estas se usarían con una función de pérdida (como `NLLLoss`) para entrenar el modelo.\n","*   **¡Experimenta!** Cambia los hiperparámetros (`N_LAYERS`, `D_MODEL`, `NUM_HEADS`), los tamaños de secuencia o batch y observa cómo afecta (aunque aquí solo hacemos un forward pass).\n","\n","---\n"],"metadata":{"id":"mRnfqRFvD_Tj"}},{"cell_type":"markdown","source":["## Comparación Práctica: Transformers vs. RNNs/CNNs\n","\n","Antes de los Transformers, las Redes Neuronales Recurrentes (RNNs, como LSTMs y GRUs) y las Redes Neuronales Convolucionales (CNNs) eran las arquitecturas dominantes en NLP.\n","\n","*   **RNNs:** Procesan secuencias token por token, manteniendo un estado oculto que (idealmente) captura información de tokens anteriores. Son inherentemente secuenciales.\n","*   **CNNs:** Aplican filtros (convoluciones) sobre ventanas de tokens, capturando patrones locales. Se pueden apilar para capturar contextos más amplios, pero no son tan naturales para dependencias a largo plazo como las RNNs.\n","\n","**Ventajas Prácticas de los Transformers:**\n","\n","1.  **Paralelización Superior:** A diferencia del procesamiento secuencial de las RNNs, los cálculos dentro de un Transformer (especialmente la auto-atención y las capas feed-forward) pueden paralelizarse masivamente a lo largo de la dimensión de la secuencia. Esto permite entrenar modelos mucho más grandes en hardware moderno (GPUs/TPUs) de manera significativamente más rápida que RNNs equivalentes en tamaño.\n","2.  **Mejor Captura de Dependencias a Largo Plazo:** La auto-atención calcula directamente las interacciones entre cualquier par de tokens en la secuencia, sin importar su distancia. Esto supera la dificultad de las RNNs para propagar información a través de muchos pasos de tiempo (problema del desvanecimiento del gradiente), permitiendo a los Transformers modelar relaciones complejas en textos largos de manera más efectiva.\n","3.  **Representaciones Contextuales Ricas:** La auto-atención permite que la representación de cada token se base en una combinación ponderada de todos los demás tokens, generando embeddings contextuales muy potentes (ej. BERT).\n","\n","**Desventajas Prácticas de los Transformers (Estándar):**\n","\n","1.  **Complejidad Cuadrática con la Longitud de Secuencia:** La auto-atención estándar requiere calcular una puntuación para cada par de tokens, lo que resulta en una complejidad computacional y de memoria de O(n^2), donde n es la longitud de la secuencia. Esto hace que procesar secuencias muy largas (miles de tokens) sea computacionalmente muy costoso o inviable con la arquitectura original. Las RNNs tienen complejidad lineal O(n).\n","2.  **Mayor Necesidad de Datos y Cómputo (para modelos grandes):** Aunque pueden entrenarse más rápido debido a la paralelización, los modelos Transformer más potentes (como BERT, GPT) suelen ser muy grandes y requieren enormes cantidades de datos y recursos computacionales para su pre-entrenamiento.\n","3.  **Menos Inducción Secuencial Inherente:** No tienen un sesgo inductivo tan fuerte hacia el orden secuencial como las RNNs. La información posicional debe añadirse explícitamente (Codificación Posicional).\n","\n","**En resumen:** Para la mayoría de las tareas de NLP modernas, las ventajas de paralelización y captura de contexto de los Transformers superan sus desventajas, especialmente con la disponibilidad de modelos pre-entrenados y arquitecturas eficientes para secuencias largas.\n","\n","---"],"metadata":{"id":"oH0y7e74Eh3J"}},{"cell_type":"markdown","source":["## 3. Atención Práctica: Queries, Keys y Values\n","\n","El mecanismo central es la **Atención Escalada por Producto Punto (Scaled Dot-Product Attention)**. Funciona con tres vectores derivados de la entrada para cada token: Query (Q), Key (K) y Value (V).\n","\n","*   **Query (Q):** Representa el token actual que está \"preguntando\" o buscando información relevante.\n","*   **Key (K):** Representa un token en la secuencia (potencialmente todos, incluida ella misma) que \"anuncia\" la información que posee.\n","*   **Value (V):** Representa el contenido o la información real del token asociado a la Key.\n","\n","**Flujo de Cálculo:**\n","\n","1.  **Scores:** Calcula qué tan relevante es cada Key para una Query dada. Se hace mediante el producto punto: `Score = Q * K^T`.\n","2.  **Escalado:** Divide los scores por la raíz cuadrada de la dimensión de los vectores Key (`d_k`) para estabilizar los gradientes: `Scaled_Score = Score / sqrt(d_k)`.\n","3.  **Máscara (Opcional):** Si se proporciona una máscara (ej. para ignorar padding o posiciones futuras), los scores correspondientes se establecen en un valor muy negativo (ej. -infinito) antes del softmax.\n","4.  **Ponderación (Softmax):** Aplica softmax a los scores escalados (y enmascarados) para obtener pesos de atención que suman 1. Estos pesos indican cuánta atención debe prestar la Query a cada Value: `Weights = softmax(Scaled_Score)`.\n","5.  **Salida:** Calcula la suma ponderada de los Values usando los pesos de atención: `Output = Weights * V`.\n"],"metadata":{"id":"7gP-a2OJEtxh"}},{"cell_type":"markdown","source":["Ejemplo Numérico Simple\n","\n","Veamos un ejemplo muy simplificado con un solo Query, tres Keys/Values y una dimensión `d_k` pequeña."],"metadata":{"id":"bL0IeVSzEznA"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import math\n","\n","# Dimensiones simplificadas\n","d_k = 4 # Dimensión de Key/Query/Value\n","seq_len_k = 3 # Número de Keys/Values\n","\n","# Vectores de ejemplo (Batch=1, Num_Heads=1)\n","# Suponemos que ya han pasado por las proyecciones lineales\n","query = torch.randn(1, 1, d_k) # (Batch, Seq_q=1, d_k)\n","keys = torch.randn(1, seq_len_k, d_k) # (Batch, Seq_k, d_k)\n","values = torch.randn(1, seq_len_k, d_k) # (Batch, Seq_k, d_k)\n","\n","print(f\"Query (Q) shape: {query.shape}\")\n","print(f\"Keys (K) shape: {keys.shape}\")\n","print(f\"Values (V) shape: {values.shape}\")\n","\n","# 1. Calcular Scores (Producto Punto)\n","# query: (1, 1, d_k) -> (1, d_k)\n","# keys.transpose(-2, -1): (1, d_k, Seq_k)\n","# scores: (1, 1, d_k) @ (1, d_k, Seq_k) -> (1, 1, Seq_k)\n","scores = torch.matmul(query, keys.transpose(-2, -1))\n","print(f\"\\nScores (Q @ K^T) shape: {scores.shape}\")\n","print(f\"Scores: {scores}\")\n","\n","# 2. Escalar\n","scaled_scores = scores / math.sqrt(d_k)\n","print(f\"\\nScaled Scores shape: {scaled_scores.shape}\")\n","print(f\"Scaled Scores: {scaled_scores}\")\n","\n","# 3. Máscara (Opcional) - Ejemplo: Ignorar el último Key/Value\n","# mask shape debe ser compatible para broadcast: (Batch, Seq_q, Seq_k)\n","mask = torch.tensor([[[True, True, False]]]) # (1, 1, 3) - True donde NO está enmascarado\n","print(f\"\\nMask: {mask}\")\n","scaled_scores_masked = scaled_scores.masked_fill(mask == 0, -1e9)\n","print(f\"Scaled Scores (Masked): {scaled_scores_masked}\")\n","\n","# 4. Ponderación (Softmax)\n","# Aplicamos softmax sobre la última dimensión (Seq_k)\n","attention_weights = F.softmax(scaled_scores_masked, dim=-1)\n","print(f\"\\nAttention Weights (Softmax) shape: {attention_weights.shape}\")\n","print(f\"Attention Weights: {attention_weights}\")\n","# Nota: El peso para la posición enmascarada (la última) debería ser cercano a cero.\n","\n","# 5. Salida (Suma Ponderada de Values)\n","# attention_weights: (1, 1, Seq_k)\n","# values: (1, Seq_k, d_k)\n","# output: (1, 1, Seq_k) @ (1, Seq_k, d_k) -> (1, 1, d_k)\n","output = torch.matmul(attention_weights, values)\n","print(f\"\\nOutput (Weighted Sum of V) shape: {output.shape}\")\n","print(f\"Output: {output}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GipmouWFFv_","executionInfo":{"status":"ok","timestamp":1746557898940,"user_tz":-60,"elapsed":53,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"1b5b785c-6600-4a54-c2fc-0e65f63c5a7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query (Q) shape: torch.Size([1, 1, 4])\n","Keys (K) shape: torch.Size([1, 3, 4])\n","Values (V) shape: torch.Size([1, 3, 4])\n","\n","Scores (Q @ K^T) shape: torch.Size([1, 1, 3])\n","Scores: tensor([[[-0.6446, -0.3494,  1.9690]]])\n","\n","Scaled Scores shape: torch.Size([1, 1, 3])\n","Scaled Scores: tensor([[[-0.3223, -0.1747,  0.9845]]])\n","\n","Mask: tensor([[[ True,  True, False]]])\n","Scaled Scores (Masked): tensor([[[-3.2232e-01, -1.7471e-01, -1.0000e+09]]])\n","\n","Attention Weights (Softmax) shape: torch.Size([1, 1, 3])\n","Attention Weights: tensor([[[0.4632, 0.5368, 0.0000]]])\n","\n","Output (Weighted Sum of V) shape: torch.Size([1, 1, 4])\n","Output: tensor([[[-0.6992, -0.0519,  0.6605,  1.1972]]])\n"]}]},{"cell_type":"markdown","source":["**Explicación Práctica:**\n","\n","*   El código sigue los 5 pasos descritos.\n","*   La `query` calcula su similitud (`scores`) con cada `key`.\n","*   Los `scores` se escalan y opcionalmente se enmascaran.\n","*   `softmax` convierte los scores en `attention_weights` (una distribución de probabilidad).\n","*   La `output` es una combinación de los `values`, ponderada por cuánta atención recibió cada uno. El `value` correspondiente a la `key` enmascarada contribuye muy poco o nada a la salida.\n","*   En la **Atención Multi-Cabeza**, este proceso se realiza en paralelo con diferentes proyecciones de Q, K, V (las \"cabezas\"), y los resultados se concatenan y proyectan linealmente al final.\n","\n","---"],"metadata":{"id":"SlcFZT_5FMD_"}},{"cell_type":"markdown","source":["## Modelos Preentrenados y Ajuste Fino (Fine-Tuning) con 🤗 Transformers\n","\n","Entrenar un Transformer desde cero requiere muchos datos y recursos. En la práctica, es mucho más común usar **modelos pre-entrenados** y **ajustarlos (fine-tuning)** para una tarea específica. La librería `transformers` de Hugging Face 🤗 facilita enormemente este proceso.\n","\n","**Conceptos Clave:**\n","\n","*   **Pre-entrenamiento:** Modelos enormes (como BERT, GPT, T5) se entrenan en corpus masivos de texto no etiquetado (ej. Wikipedia, libros, web) con objetivos como predecir palabras enmascaradas (BERT) o predecir la siguiente palabra (GPT). Aprenden representaciones lingüísticas generales.\n","*   **Ajuste Fino (Fine-Tuning):** Se toma un modelo pre-entrenado y se continúa entrenando (generalmente solo las últimas capas o todas con una tasa de aprendizaje baja) en un conjunto de datos más pequeño y específico de la tarea (ej. clasificación de sentimientos, respuesta a preguntas).\n","\n","**Modelos Populares:**\n","\n","*   **BERT (Bidirectional Encoder Representations from Transformers):** Basado en el codificador. Excelente para tareas de comprensión (clasificación, NER, Q&A). Aprende contexto de izquierda a derecha y de derecha a izquierda.\n","*   **GPT (Generative Pre-trained Transformer):** Basado en el decodificador. Excelente para generación de texto (escritura creativa, chatbots). Es autorregresivo (predice el siguiente token basado en los anteriores).\n","*   **T5 (Text-to-Text Transfer Transformer):** Arquitectura Encoder-Decoder. Trata todas las tareas como \"texto a texto\" añadiendo prefijos (ej. \"summarize: ...\", \"translate English to German: ...\"). Muy versátil.\n","*   **DistilBERT:** Una versión más pequeña y rápida de BERT, creada mediante destilación del conocimiento. Ideal para entornos con recursos limitados.\n"],"metadata":{"id":"nE8dszjKFqht"}},{"cell_type":"markdown","source":["### Instalación de Librerías\n","\n","Necesitamos instalar `transformers`, `datasets` (para cargar/manejar datos fácilmente) y `evaluate` (para métricas)."],"metadata":{"id":"3J79BYTHF9de"}},{"cell_type":"code","source":["# ¡Asegúrate de ejecutar esta celda!\n","!pip install transformers datasets evaluate torch accelerate -q\n","!pip install numpy==1.23.5 -q\n","\n","\n","print(\"Librerías instaladas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFEsf5DOGDV3","executionInfo":{"status":"ok","timestamp":1746559311670,"user_tz":-60,"elapsed":14954,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"034900b1-36de-479b-d016-f8a16da62f06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Librerías instaladas.\n"]}]},{"cell_type":"markdown","source":["Ejemplo Práctico: Fine-Tuning para Clasificación de Texto\n","\n","Vamos a ajustar DistilBERT (una versión más ligera de BERT) para una tarea de clasificación de sentimientos usando un pequeño dataset de ejemplo.\n","\n","**1. Cargar Dataset y Tokenizador**\n","\n","Usaremos un dataset de ejemplo simple. En una aplicación real, usarías `datasets.load_dataset(\"nombre_del_dataset\")`."],"metadata":{"id":"s3Wsh0D5GNNH"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from datasets import Dataset\n","import torch\n","import numpy as np\n","import evaluate\n","\n","# Modelo pre-entrenado a usar (ligero y rápido)\n","model_name = \"distilbert-base-uncased\"\n","\n","# Cargar tokenizador asociado al modelo\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Datos de ejemplo (muy pequeños para demostración)\n","texts = [\"I love this movie, it's fantastic!\",\n","         \"This was the worst film I have ever seen.\",\n","         \"The acting was okay, but the plot was boring.\",\n","         \"Absolutely brilliant, a must-watch!\",\n","         \"I didn't like it very much.\",\n","         \"A masterpiece of cinema.\"]\n","labels = [1, 0, 0, 1, 0, 1] # 1: Positivo, 0: Negativo\n","\n","# Crear un Dataset de Hugging Face\n","data = {\"text\": texts, \"label\": labels}\n","dataset = Dataset.from_dict(data)\n","\n","# Dividir en entrenamiento y evaluación (simple split para ejemplo)\n","dataset = dataset.train_test_split(test_size=0.3) # 70% train, 30% test\n","\n","print(\"Dataset de ejemplo:\")\n","print(dataset)\n","\n","# Función para tokenizar los textos\n","def tokenize_function(examples):\n","    # padding='max_length' asegura que todas las secuencias tengan la misma longitud\n","    # truncation=True corta secuencias más largas que max_length\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n","\n","# Aplicar tokenización a todo el dataset\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","\n","# Eliminar la columna de texto original, ya no la necesitamos\n","tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n","# Renombrar 'label' a 'labels' (esperado por el Trainer)\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","# Establecer el formato a tensores de PyTorch\n","tokenized_datasets.set_format(\"torch\")\n","\n","print(\"\\nDataset tokenizado:\")\n","print(tokenized_datasets)\n","print(\"\\nEjemplo de entrada tokenizada:\")\n","# Accessing the first element without slicing\n","print(tokenized_datasets[\"train\"][0])\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"6-q5bCjUGSkk","executionInfo":{"status":"error","timestamp":1746559341975,"user_tz":-60,"elapsed":25992,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"d556f78a-a08d-40cd-a480-8e32ee7f19a3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m from .pytorch_utils import (  # noqa: F401\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_deformable_detr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scipy_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpsSet\u001b[0m \u001b[0;31m# line: 170\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m \u001b[0;31m# line: 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m \u001b[0;31m# line: 263\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from jax._src.core import (\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mAbstractToken\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAbstractToken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0m_string_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'StringDType'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mxla_extension_version\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m311\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m   \u001b[0m_string_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringDType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'dtypes'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e9b30d1bc53b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'"]}]},{"cell_type":"markdown","source":["**2. Cargar Modelo Pre-entrenado**\n","\n","Cargamos DistilBERT pre-entrenado, pero especificamos que es para clasificación de secuencias y el número de etiquetas (2 en nuestro caso: positivo/negativo).\n"],"metadata":{"id":"SpTNLnVRG6Zu"}},{"cell_type":"code","source":["# Cargar el modelo pre-entrenado para clasificación de secuencias\n","# num_labels indica cuántas clases de salida hay\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","print(f\"Modelo {model_name} cargado para clasificación con {model.config.num_labels} etiquetas.\")\n"],"metadata":{"id":"dPku1k9vG88V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Definir Métricas y Argumentos de Entrenamiento**\n","\n","Necesitamos una función para calcular métricas durante la evaluación y definir los hiperparámetros para el `Trainer`."],"metadata":{"id":"t3rzdbLGHCnk"}},{"cell_type":"code","source":["# Cargar métrica de evaluación (accuracy es común para clasificación)\n","metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    # Obtener predicciones tomando el índice con mayor logit\n","    predictions = np.argmax(logits, axis=-1)\n","    # Calcular la métrica\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","# Definir argumentos para el entrenamiento\n","training_args = TrainingArguments(\n","    output_dir=\"./results_classification\", # Directorio donde guardar resultados/checkpoints\n","    evaluation_strategy=\"epoch\",         # Evaluar al final de cada época\n","    num_train_epochs=3,                  # Número de épocas (pocas para ejemplo rápido)\n","    per_device_train_batch_size=2,       # Tamaño de batch pequeño para ejemplo\n","    per_device_eval_batch_size=2,\n","    warmup_steps=1,                      # Pasos de calentamiento (pocos para ejemplo)\n","    weight_decay=0.01,\n","    logging_dir=\"./logs_classification\",   # Directorio para logs\n","    logging_steps=1,\n","    # load_best_model_at_end=True, # Opcional: cargar el mejor modelo al final\n","    # push_to_hub=False, # Opcional: subir a Hugging Face Hub\n",")\n","\n","print(\"Argumentos de entrenamiento definidos.\")\n"],"metadata":{"id":"nL8KOrk1HGur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. Crear y Ejecutar el Trainer**\n","\n","El `Trainer` de Hugging Face se encarga del bucle de entrenamiento y evaluación."],"metadata":{"id":"YNXj4XOVHRtb"}},{"cell_type":"code","source":["# Crear el objeto Trainer\n","trainer = Trainer(\n","    model=model,                         # El modelo a entrenar\n","    args=training_args,                  # Argumentos de entrenamiento\n","    train_dataset=tokenized_datasets[\"train\"], # Dataset de entrenamiento\n","    eval_dataset=tokenized_datasets[\"test\"],  # Dataset de evaluación\n","    compute_metrics=compute_metrics,     # Función para calcular métricas\n","    tokenizer=tokenizer,                 # Tokenizador (útil para padding dinámico si no se hizo antes)\n",")\n","\n","print(\"Trainer creado. Iniciando fine-tuning...\")\n","# Iniciar el entrenamiento (ajuste fino)\n","trainer.train()\n","\n","print(\"\\nFine-tuning completado.\")\n","\n","# Evaluar el modelo final\n","eval_results = trainer.evaluate()\n","print(\"\\nResultados de la evaluación final:\")\n","print(eval_results)\n"],"metadata":{"id":"m6pI9h-gHWsd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5. Guardar Modelo y Hacer Inferencia**\n","\n","Una vez entrenado, puedes guardar tu modelo ajustado y usarlo para predecir en nuevos datos."],"metadata":{"id":"jSlwtv9wHcbE"}},{"cell_type":"code","source":["# Guardar el modelo ajustado y el tokenizador\n","save_directory = \"./fine_tuned_distilbert_classifier\"\n","print(f\"\\nGuardando modelo en {save_directory}...\")\n","trainer.save_model(save_directory)\n","# El tokenizador también se guarda automáticamente con save_model si se proporcionó al Trainer\n","# tokenizer.save_pretrained(save_directory) # Opcional si no se pasó al Trainer\n","print(\"Modelo guardado.\")\n","\n","# --- Cargar y usar para inferencia --- #\n","print(\"\\nCargando modelo guardado para inferencia...\")\n","# Cargar el modelo y tokenizador guardados\n","loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n","loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n","\n","# Mover modelo a GPU si está disponible (importante para inferencia rápida)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loaded_model.to(device)\n","loaded_model.eval() # Poner en modo evaluación\n","\n","# Texto nuevo para clasificar\n","new_text = \"This movie was incredibly moving and well-acted.\"\n","print(f\"\\nClasificando nuevo texto: '{new_text}'\")\n","\n","# Tokenizar el nuevo texto\n","inputs = loaded_tokenizer(new_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","# Mover inputs a la misma GPU/CPU que el modelo\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","# Hacer la predicción\n","with torch.no_grad(): # No necesitamos gradientes para inferencia\n","    outputs = loaded_model(**inputs)\n","    logits = outputs.logits\n","\n","# Obtener la clase predicha\n","predicted_class_id = torch.argmax(logits, dim=-1).item()\n","\n","# Mapear ID a etiqueta (según nuestro dataset original)\n","predicted_label = \"Positivo\" if predicted_class_id == 1 else \"Negativo\"\n","\n","print(f\"Predicción: {predicted_label} (ID: {predicted_class_id})\")\n","\n","# Ejemplo con otro texto\n","new_text_2 = \"A complete waste of time and money.\"\n","print(f\"\\nClasificando nuevo texto: '{new_text_2}'\")\n","inputs_2 = loaded_tokenizer(new_text_2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n","with torch.no_grad():\n","    outputs_2 = loaded_model(**inputs_2)\n","    logits_2 = outputs_2.logits\n","predicted_class_id_2 = torch.argmax(logits_2, dim=-1).item()\n","predicted_label_2 = \"Positivo\" if predicted_class_id_2 == 1 else \"Negativo\"\n","print(f\"Predicción: {predicted_label_2} (ID: {predicted_class_id_2})\")\n"],"metadata":{"id":"9FK_-CecHnUS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explicación Práctica:**\n","\n","*   Hemos usado `transformers` y `datasets` para cargar un modelo pre-entrenado (DistilBERT), preparar un dataset simple y tokenizarlo.\n","*   Configuramos `TrainingArguments` y `Trainer` para manejar el bucle de fine-tuning y evaluación.\n","*   Entrenamos el modelo por unas pocas épocas en nuestros datos específicos.\n","*   Guardamos el modelo ajustado.\n","*   Finalmente, cargamos el modelo guardado y lo usamos para clasificar nuevos textos.\n","*   **¡Experimenta!** Prueba con otros modelos pre-entrenados (`bert-base-uncased`, `roberta-base`), cambia los hiperparámetros de entrenamiento, usa un dataset más grande de Hugging Face Hub (ej. `imdb`).\n","\n","---\n"],"metadata":{"id":"7tO9CujPHuJr"}},{"cell_type":"markdown","source":["## 5. Limitaciones y Consideraciones Éticas (Prácticas)\n","\n","Si bien los Transformers son potentes, es crucial ser consciente de sus limitaciones prácticas y las implicaciones éticas al usarlos:\n","\n","**Limitaciones Prácticas:**\n","\n","*   **Recursos Computacionales:** Entrenar modelos grandes desde cero o incluso ajustar modelos muy grandes requiere GPUs/TPUs potentes y tiempo considerable. La inferencia también puede ser costosa para modelos grandes.\n","*   **Longitud de Secuencia:** La complejidad cuadrática de la atención estándar limita la longitud de secuencia que se puede procesar eficientemente (aunque existen variantes como Longformer).\n","*   **\"Alucinaciones\" y Falta de Sentido Común:** Los modelos pueden generar texto que suena plausible pero es incorrecto factualmente o carece de sentido común. No \"entienden\" el mundo real, solo patrones estadísticos.\n","*   **Interpretabilidad:** Es difícil saber exactamente *por qué* un Transformer genera una salida específica, lo que es problemático en aplicaciones críticas.\n","\n","**Consideraciones Éticas Prácticas:**\n","\n","*   **Sesgo (Bias):** Los modelos aprenden sesgos (sociales, de género, raciales) presentes en los datos de entrenamiento masivos. Esto puede llevar a resultados injustos o discriminatorios. *Acción Práctica:* Audita tus modelos en busca de sesgos, usa datasets más equilibrados si es posible, y considera técnicas de mitigación de sesgos.\n","*   **Desinformación:** La capacidad de generar texto realista puede usarse para crear noticias falsas, spam, etc. *Acción Práctica:* Sé responsable con el uso de modelos generativos y considera implementar salvaguardas.\n","*   **Consumo Energético:** Entrenar modelos grandes tiene una huella de carbono significativa. *Acción Práctica:* Prefiere usar modelos pre-entrenados y ajustarlos, considera modelos más pequeños/eficientes (como DistilBERT) o técnicas de optimización (ver Sección 7) cuando sea posible.\n","*   **Privacidad:** Existe un riesgo (aunque bajo para modelos grandes) de que los modelos memoricen datos sensibles del entrenamiento. *Acción Práctica:* Asegúrate de que los datos de entrenamiento estén adecuadamente anonimizados, especialmente si son sensibles.\n","\n","---\n"],"metadata":{"id":"0WS9uyALHyiH"}},{"cell_type":"markdown","source":["## 6. Aplicaciones en Big Data (Conceptual Práctico)\n","\n","¿Cómo aplicaríamos Transformers a conjuntos de datos masivos (Big Data), como millones de registros médicos electrónicos (RME) para detectar eventos adversos a medicamentos?\n","\n","**Pasos Prácticos Conceptuales:**\n","\n","1.  **Infraestructura:** Necesitarás un entorno de computación distribuida (ej. clústeres en la nube como AWS SageMaker, Google AI Platform, Azure ML) con acceso a múltiples GPUs/TPUs.\n","2.  **Almacenamiento de Datos:** Los datos (RME de-identificados) probablemente residirán en un data lake o almacén de datos distribuido (ej. S3, GCS, HDFS).\n","3.  **Preprocesamiento Distribuido:** Usar frameworks como **Apache Spark** (con PySpark) o **Dask** para:\n","    *   Leer datos en paralelo desde el almacenamiento distribuido.\n","    *   Realizar limpieza, de-identificación (¡crucial!), y tokenización distribuida (posiblemente usando `datasets.map` con procesamiento distribuido si se integra con Spark/Dask, o UDFs de Spark con el tokenizador).\n","    *   Guardar los datos preprocesados/tokenizados de nuevo en formato eficiente (ej. Parquet).\n","4.  **Entrenamiento Distribuido:** Utilizar librerías que se integren con PyTorch para paralelizar el entrenamiento del Transformer (ej. ajuste fino de ClinicalBERT) en múltiples GPUs/nodos:\n","    *   **PyTorch DistributedDataParallel (DDP):** Estándar de PyTorch para paralelismo de datos.\n","    *   **Hugging Face Accelerate:** Simplifica el uso de DDP y otras estrategias (como DeepSpeed) con los modelos y `Trainer` de Transformers.\n","    *   **Horovod:** Otro framework popular para entrenamiento distribuido.\n","    *   **DeepSpeed:** Librería de Microsoft para entrenar modelos masivos, optimizando memoria y velocidad (paralelismo de datos, tensor, pipeline, optimizador ZeRO).\n","    *   **Ejemplo con Accelerate/Trainer:** El `Trainer` de Hugging Face se integra con `accelerate`. Simplemente lanzando tu script de entrenamiento con `accelerate launch tu_script.py --args...` en un entorno configurado, `accelerate` maneja la distribución.\n","5.  **Inferencia Distribuida:** Para aplicar el modelo entrenado a nuevos datos a gran escala, también se pueden usar frameworks distribuidos (Spark UDFs con el modelo cargado, Dask) o servicios de inferencia optimizados (ej. NVIDIA Triton Inference Server, SageMaker Endpoints).\n","6.  **Monitorización y Gestión:** Usar herramientas para monitorizar el uso de recursos, el progreso del entrenamiento y gestionar los experimentos (ej. MLflow, Weights & Biases).\n","\n","**Consideraciones de Código:**\n","\n","*   **Manejo de Memoria:** Cargar modelos grandes y batches de datos requiere GPUs con mucha VRAM. Técnicas como gradient accumulation (en `TrainingArguments`), precisión mixta (`fp16=True`), y DeepSpeed (ZeRO) son esenciales.\n","*   **Eficiencia I/O:** Leer datos eficientemente desde el almacenamiento distribuido es clave. Formatos como Parquet o TFRecord son preferibles a archivos de texto plano.\n","*   **Robustez:** El código debe manejar fallos transitorios en el clúster (ej. reintentos, checkpoints).\n"],"metadata":{"id":"h5rSAwCFH0Fr"}},{"cell_type":"code","source":["# Ejemplo conceptual (No ejecutable directamente aquí)\n","# Asume que tienes un script 'train_ehr.py' que usa Hugging Face Trainer\n","\n","# Configurar accelerate (una vez por nodo/máquina)\n","# !accelerate config\n","\n","# Lanzar entrenamiento distribuido (ej. en 4 GPUs)\n","# !accelerate launch --num_processes=4 train_ehr.py \\\n","#   --output_dir ./ehr_output \\\n","#   --model_name_or_path emilyalsentzer/Bio_ClinicalBERT \\\n","#   --train_file path/to/distributed/train.parquet \\\n","#   --validation_file path/to/distributed/eval.parquet \\\n","#   --do_train --do_eval \\\n","#   --num_train_epochs 1 \\\n","#   --per_device_train_batch_size 8 \\\n","#   --fp16 # Usar precisión mixta\n","\n","print(\"Conceptual: Lanzamiento de entrenamiento distribuido con Accelerate.\")\n"],"metadata":{"id":"bGVe3jCeH6Kc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Desafíos Clave:** Privacidad (HIPAA/GDPR), heterogeneidad de datos médicos, coste computacional, interpretabilidad.\n","\n","---\n"],"metadata":{"id":"-LKD2krDH_lK"}},{"cell_type":"markdown","source":["## 7. Optimización y Eficiencia (Práctica Conceptual)\n","\n","Los modelos Transformer grandes pueden ser lentos y consumir mucha memoria. Aquí hay un vistazo práctico a cómo optimizarlos:\n","\n","**Técnicas Comunes:**\n","\n","*   **Cuantización (Quantization):** Reduce la precisión de los números (pesos/activaciones) de 32 bits (FP32) a 16 bits (FP16/BF16) o 8 bits (INT8). Esto reduce el tamaño del modelo (~4x para INT8) y acelera la inferencia en hardware compatible (~2-4x).\n","*   **Poda (Pruning):** Elimina pesos o estructuras menos importantes del modelo. Puede reducir significativamente el tamaño y los cálculos, a veces con poca pérdida de precisión.\n","*   **Destilación del Conocimiento (Knowledge Distillation):** Entrena un modelo más pequeño (\"estudiante\") para imitar a uno más grande (\"profesor\"). El estudiante aprende de las predicciones del profesor, logrando un buen rendimiento con menos tamaño/velocidad (ej. DistilBERT).\n","*   **Arquitecturas Eficientes:** Usar variantes de Transformer diseñadas para ser más rápidas o manejar secuencias largas (ej. Longformer, Linformer, Reformer).\n","\n","### 7.1. Ejemplo Práctico: Cuantización con PyTorch\n","\n","PyTorch ofrece herramientas para cuantización post-entrenamiento (PTQ) y entrenamiento consciente de la cuantización (QAT)."],"metadata":{"id":"RXp3a8OIIE9c"}},{"cell_type":"code","source":["import torch\n","import torch.quantization\n","import copy\n","\n","# --- Cuantización Dinámica Post-Entrenamiento (PTQ) ---\n","# Más simple, buena para LSTMs/Transformers, cuantiza solo pesos\n","\n","# Carga tu modelo FP32 entrenado (usaremos una red lineal simple como ejemplo)\n","# En la práctica, cargarías tu modelo Transformer ajustado\n","fp32_model = nn.Sequential(\n","    nn.Linear(128, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, 10)\n",")\n","fp32_model.eval() # Poner en modo evaluación\n","\n","# Aplicar cuantización dinámica (para capas Lineales y RNNs)\n","# backend=\\'qnnpack\\' es común para ARM, \\'fbgemm\\' para x86\n","quantized_dynamic_model = torch.quantization.quantize_dynamic(\n","    fp32_model, {nn.Linear}, dtype=torch.qint8, inplace=False\n",")\n","\n","print(\"Modelo original (FP32):\")\n","# print(fp32_model)\n","print(\"\\nModelo cuantizado dinámicamente (INT8 pesos para Lineales):\")\n","# print(quantized_dynamic_model)\n","\n","# Comparar tamaño (conceptual)\n","# import os\n","# torch.save(fp32_model.state_dict(), \"fp32_model.pth\")\n","# torch.save(quantized_dynamic_model.state_dict(), \"quantized_dynamic_model.pth\")\n","# print(f\"Tamaño FP32: {os.path.getsize(\\'fp32_model.pth\\') / 1e6:.2f} MB\")\n","# print(f\"Tamaño INT8 Dinámico: {os.path.getsize(\\'quantized_dynamic_model.pth\\') / 1e6:.2f} MB\") # Debería ser ~1/4 para pesos lineales\n","\n","# --- Cuantización Estática Post-Entrenamiento (PTQ) ---\n","# Requiere calibración con datos, cuantiza pesos y activaciones\n","\n","# Clona el modelo original\n","static_model = copy.deepcopy(fp32_model)\n","static_model.eval()\n","\n","# Especificar configuración de cuantización\n","static_model.qconfig = torch.quantization.get_default_qconfig(\\'fbgemm\\') # o \\'qnnpack\\'\n","\n","# Fusionar módulos (Conv-BN-ReLU, etc.) si aplica (no en este ejemplo simple)\n","# static_model_fused = torch.quantization.fuse_modules(static_model, [['0', '1']], inplace=False)\n","\n","# Preparar el modelo para la calibración\n","static_model_prepared = torch.quantization.prepare(static_model, inplace=False)\n","\n","# Calibrar con datos representativos (ej. algunos batches del dataset de validación)\n","# print(\"\\nCalibrando modelo estático...\")\n","# with torch.no_grad():\n","#     for i in range(10): # Usar datos reales aquí\n","#         dummy_input = torch.randn(1, 128)\n","#         static_model_prepared(dummy_input)\n","\n","# Convertir a modelo cuantizado\n","# static_model_quantized = torch.quantization.convert(static_model_prepared, inplace=False)\n","# print(\"Modelo cuantizado estáticamente (INT8 pesos y activaciones):\")\n","# print(static_model_quantized)\n","\n","# --- Entrenamiento Consciente de Cuantización (QAT) ---\n","# Mejor precisión, simula cuantización durante el entrenamiento\n","\n","# qat_model = copy.deepcopy(fp32_model)\n","# qat_model.train() # Poner en modo entrenamiento\n","# qat_model.qconfig = torch.quantization.get_default_qat_qconfig(\\'fbgemm\\')\n","# qat_model_fused = torch.quantization.fuse_modules(qat_model, [['0', '1']], inplace=False)\n","# qat_model_prepared = torch.quantization.prepare_qat(qat_model_fused, inplace=False)\n","\n","# Entrenar el modelo preparado (qat_model_prepared) por algunas épocas...\n","# print(\"\\n(Conceptual) Entrenando con QAT...\")\n","\n","# Después del entrenamiento, convertir a modelo cuantizado\n","# qat_model_prepared.eval()\n","# qat_model_quantized = torch.quantization.convert(qat_model_prepared, inplace=False)\n","# print(\"(Conceptual) Modelo final cuantizado con QAT:\")\n","# print(qat_model_quantized)\n","\n","print(\"\\nEjemplos conceptuales de cuantización con PyTorch.\")\n"],"metadata":{"id":"qIeVjsiZIM-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.2. Hugging Face Optimum\n","\n","La librería `optimum` de Hugging Face simplifica la aplicación de técnicas de optimización (cuantización, ONNX Runtime) a los modelos de `transformers`."],"metadata":{"id":"h1FYQ0PnIR4r"}},{"cell_type":"code","source":["# !pip install optimum[onnxruntime]\n","\n","from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n","from optimum.onnxruntime.configuration import AutoQuantizationConfig\n","\n","# Directorio del modelo ajustado de la sección anterior\n","model_checkpoint_dir = \"./fine_tuned_distilbert_classifier\"\n","# Directorio de salida para el modelo ONNX cuantizado\n","onnx_quantized_output_dir = \"./onnx_quantized_distilbert\"\n","\n","# --- Cuantización Post-Entrenamiento con Optimum --- #\n","\n","# 1. Crear cuantizador desde el modelo ajustado\n","ort_quantizer = ORTQuantizer.from_pretrained(model_checkpoint_dir)\n","\n","# 2. Definir configuración de cuantización (ej. AVX2 para CPU, INT8)\n","# qconfig = AutoQuantizationConfig.avx2(config=None, is_static=False) # Dinámica\n","qconfig = AutoQuantizationConfig.avx2(config=None, is_static=True) # Estática (requiere dataset de calibración)\n","\n","# 3. (Solo para estática) Cargar dataset de calibración\n","# from datasets import load_dataset\n","# calibration_dataset = ort_quantizer.get_calibration_dataset(\n","#     \"glue\", # Ejemplo: dataset GLUE\n","#     dataset_config_name=\"sst2\", # Ejemplo: tarea SST-2\n","#     preprocess_function=lambda examples: tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True),\n","#     num_samples=50, # Número de muestras para calibrar\n","#     dataset_split=\"train\",\n","# )\n","\n","# 4. Cuantizar a formato ONNX\n","# Para estática, añadir: calibration_dataset=calibration_dataset\n","ort_quantizer.quantize(\n","    save_dir=onnx_quantized_output_dir,\n","    quantization_config=qconfig,\n",")\n","\n","print(f\"Modelo cuantizado y exportado a ONNX en {onnx_quantized_output_dir}\")\n","\n","# --- Cargar y usar modelo ONNX cuantizado --- #\n","# print(\"\\nCargando modelo ONNX cuantizado...\")\n","# loaded_onnx_model = ORTModelForSequenceClassification.from_pretrained(onnx_quantized_output_dir)\n","# loaded_onnx_tokenizer = AutoTokenizer.from_pretrained(onnx_quantized_output_dir)\n","\n","# new_text = \"This is a great tool!\"\n","# inputs = loaded_onnx_tokenizer(new_text, return_tensors=\"pt\")\n","\n","# outputs = loaded_onnx_model(**inputs)\n","# logits = outputs.logits\n","# predicted_class_id = torch.argmax(logits, dim=-1).item()\n","# print(f\"Predicción ONNX: {\"Positivo\" if predicted_class_id == 1 else \"Negativo\"}\")\n","\n"],"metadata":{"id":"bf38KBM2IW-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Explicación Práctica:**\n","\n","*   PyTorch ofrece APIs para cuantización dinámica, estática y QAT. La dinámica es la más simple, la estática requiere calibración, y QAT da la mejor precisión pero requiere reentrenamiento.\n","*   Hugging Face `optimum` simplifica la cuantización (especialmente a formato ONNX para inferencia acelerada con ONNX Runtime) de modelos `transformers`.\n","*   La elección depende del hardware objetivo (CPU/GPU/TPU), los requisitos de precisión y la complejidad de implementación aceptable.\n","\n","---"],"metadata":{"id":"tyqMR42wIchK"}},{"cell_type":"markdown","source":["## 8. Diseño Personalizado: Resumen Abstractivo con 🤗 Transformers\n","\n","Diseñar un modelo \"personalizado\" a menudo significa adaptar una arquitectura pre-entrenada para una tarea específica. Vamos a usar un modelo pre-entrenado Encoder-Decoder (como BART o PEGASUS) para **resumen abstractivo** (generar un resumen que no solo copia frases).\n","\n","**Tarea:** Dado un texto largo (ej. artículo), generar un resumen corto.\n","\n","**Modelo:** Usaremos `google/pegasus-xsum`, un modelo pre-entrenado específicamente para resúmenes cortos y abstractivos (entrenado en el dataset XSum).\n"],"metadata":{"id":"EFRaQth7Ipra"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","\n","# Cargar modelo y tokenizador pre-entrenado para resumen\n","model_name_summarization = \"google/pegasus-xsum\"\n","print(f\"Cargando modelo {model_name_summarization}...\")\n","\n","# Usar try-except por si hay problemas de conexión o memoria en Colab\n","try:\n","    tokenizer_summarization = AutoTokenizer.from_pretrained(model_name_summarization)\n","    model_summarization = AutoModelForSeq2SeqLM.from_pretrained(model_name_summarization)\n","\n","    # Mover a GPU si está disponible\n","    device_sum = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model_summarization.to(device_sum)\n","    model_summarization.eval() # Modo evaluación\n","\n","    print(\"Modelo de resumen cargado.\")\n","\n","    # Texto de ejemplo para resumir (puedes reemplazarlo)\n","    ARTICLE_TO_SUMMARIZE = (\"\"\"\n","    Scientists have discovered a new species of deep-sea fish living near hydrothermal vents\n","    in the Pacific Ocean. The fish, temporarily named Ventichthys чудо (chudo, meaning wonder),\n","    possesses unique bioluminescent features never before seen. Adapted to extreme pressure\n","    and temperature, its discovery challenges existing theories about the limits of life\n","    on Earth. Researchers used a remotely operated vehicle (ROV) to capture footage and\n","    samples at depths exceeding 3,000 meters. Further genetic analysis is underway to\n","    understand its evolutionary lineage and unique adaptations. The finding highlights\n","    how much of the deep ocean remains unexplored and the potential for discovering\n","    entirely new ecosystems and life forms.\n","    \"\"\")\n","\n","    print(f\"\\nTexto a resumir:\\n{ARTICLE_TO_SUMMARIZE}\")\n","\n","    # Tokenizar el texto de entrada\n","    inputs_summarization = tokenizer_summarization(ARTICLE_TO_SUMMARIZE,\n","                                                   max_length=1024, # Longitud máxima de entrada para PEGASUS\n","                                                   return_tensors=\"pt\",\n","                                                   truncation=True).to(device_sum)\n","\n","    # Generar el resumen\n","    # Parámetros de generación comunes:\n","    # - max_length: Longitud máxima del resumen generado\n","    # - min_length: Longitud mínima del resumen generado\n","    # - num_beams: Usa beam search para mejor calidad (más lento)\n","    # - length_penalty: Penaliza resúmenes más largos/cortos\n","    # - no_repeat_ngram_size: Evita repetir n-gramas\n","    print(\"\\nGenerando resumen...\")\n","    with torch.no_grad():\n","        summary_ids = model_summarization.generate(inputs_summarization[\"input_ids\"],\n","                                                 num_beams=4,\n","                                                 min_length=30,\n","                                                 max_length=60, # Resumen corto para XSum\n","                                                 early_stopping=True)\n","\n","    # Decodificar los IDs generados a texto\n","    generated_summary = tokenizer_summarization.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    print(f\"\\nResumen Generado:\\n{generated_summary}\")\n","\n","except Exception as e:\n","    print(f\"Error al cargar o usar el modelo de resumen: {e}\")\n","    print(\"Esto puede deberse a memoria insuficiente en Colab o problemas de red.\")\n","    print(\"Intenta reiniciar el entorno de ejecución o usar un modelo más pequeño si es necesario.\")\n","\n"],"metadata":{"id":"z_k-HNHmIq9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Explicación Práctica:**\n","\n","*   Cargamos un modelo Seq2Seq (`AutoModelForSeq2SeqLM`) y su tokenizador, específicamente `pegasus-xsum` que está optimizado para resúmenes cortos.\n","*   Tokenizamos el artículo de entrada.\n","*   Usamos el método `model.generate()` para producir el resumen. Este método implementa la decodificación (a menudo usando beam search) para generar la secuencia de salida token por token.\n","*   Ajustamos los parámetros de `generate` (como `num_beams`, `max_length`, `min_length`) para controlar la calidad y longitud del resumen.\n","*   Finalmente, decodificamos los IDs de token generados para obtener el resumen en texto.\n","*   **¡Experimenta!** Prueba con diferentes textos, ajusta los parámetros de `generate`, o incluso intenta con otro modelo pre-entrenado para resumen (ej. `google/bart-large-cnn`).\n","\n","---\n","\n","## Conclusión y Próximos Pasos\n","\n","Este cuaderno ha proporcionado una introducción práctica a los Transformers con PyTorch, cubriendo desde la implementación básica hasta el uso avanzado con modelos pre-entrenados y técnicas de optimización.\n","\n","**Puntos Clave:**\n","\n","*   Los Transformers se basan en la **atención** para capturar relaciones en los datos.\n","*   Son altamente **paralelizables**, permitiendo modelos muy grandes.\n","*   El **pre-entrenamiento y ajuste fino** con librerías como 🤗 Transformers es el enfoque estándar.\n","*   Existen **limitaciones** (recursos, longitud de secuencia) y **consideraciones éticas** (sesgo, desinformación) importantes.\n","*   Las técnicas de **optimización** (cuantización, poda) y las **arquitecturas eficientes** ayudan a desplegar modelos en entornos prácticos.\n","\n","**Próximos Pasos:**\n","\n","*   Profundiza en tareas específicas (Q&A, NER, Traducción) usando `transformers`.\n","*   Explora diferentes modelos pre-entrenados en [Hugging Face Hub](https://huggingface.co/models).\n","*   Aprende más sobre técnicas de optimización con `optimum` o PyTorch.\n","*   Intenta entrenar un modelo desde cero (si tienes suficientes datos y recursos) o ajustar modelos más grandes.\n","*   Contribuye a la comunidad de código abierto.\n","\n","¡Esperamos que esta guía práctica te haya sido útil en tu viaje de aprendizaje sobre Transformers!\n","\n","---\n","\n","## Referencias (Simplificado)\n","\n","*   Vaswani et al. (2017). Attention is all you need. [Link](https://arxiv.org/abs/1706.03762)\n","*   Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers... [Link](https://arxiv.org/abs/1810.04805)\n","*   Radford et al. (GPT series). OpenAI Blog.\n","*   Raffel et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. [Link](https://arxiv.org/abs/1910.10683)\n","*   Wolf et al. (2019). HuggingFace's Transformers: State-of-the-Art Natural Language Processing. [Link](https://arxiv.org/abs/1910.03771)\n","*   Zhang et al. (2020). PEGASUS: Pre-training with Extracted Gap-sentences... [Link](https://arxiv.org/abs/1912.08777)\n","*   PyTorch Documentation: [Link](https://pytorch.org/docs/stable/index.html)\n","*   Hugging Face Documentation (Transformers, Datasets, Evaluate, Optimum): [Link](https://huggingface.co/docs)\n","\n","---\n","\n"],"metadata":{"id":"WD4cDXtNIvHC"}},{"cell_type":"markdown","source":["## Otra práctica"],"metadata":{"id":"TuFA1pPohBmF"}},{"cell_type":"markdown","source":["La estructura base del Transformer sencillo en PyTorch"],"metadata":{"id":"UlnQEZCwhSdA"}},{"cell_type":"code","source":["# Importamos librerias\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Clase MultiHeadAttention: Implementa la atención multi-cabeza\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        # Verificamos que d_model sea divisible por el número de cabezas\n","        assert d_model % n_heads == 0\n","        self.d_k = d_model // n_heads  # Dimensión de cada cabeza\n","        self.n_heads = n_heads\n","\n","        # Definimos capas lineales para Q, K, V y la salida\n","        self.q_linear = nn.Linear(d_model, d_model)\n","        self.k_linear = nn.Linear(d_model, d_model)\n","        self.v_linear = nn.Linear(d_model, d_model)\n","        self.out_linear = nn.Linear(d_model, d_model)\n","\n","    def attention(self, Q, K, V):\n","        # Calculamos los scores de atención escalados\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n","        # Aplicamos softmax para obtener los pesos de atención\n","        attn_weights = F.softmax(scores, dim=-1)\n","        # Calculamos la salida ponderada\n","        output = torch.matmul(attn_weights, V)\n","        return output, attn_weights\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Aplicamos las capas lineales para obtener Q, K, V\n","        Q = self.q_linear(x)\n","        K = self.k_linear(x)\n","        V = self.v_linear(x)\n","\n","        # Redimensionamos para aplicar atención multi-cabeza\n","        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n","        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n","        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n","\n","        # Llamamos a la función de atención\n","        attn_output, _ = self.attention(Q, K, V)\n","\n","        # Volvemos a unir las cabezas de atención\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_k * self.n_heads)\n","        # Aplicamos la capa de salida\n","        output = self.out_linear(attn_output)\n","        return output\n","\n","# Clase FeedForward: Implementa el MLP del Transformer\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model, ff_dim, dropout=0.1):\n","        super(FeedForward, self).__init__()\n","        # Capa intermedia con ReLU\n","        self.fc1 = nn.Linear(d_model, ff_dim)\n","        # Capa de salida\n","        self.fc2 = nn.Linear(ff_dim, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # Aplicamos ReLU y Dropout\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        # Capa de salida\n","        x = self.fc2(x)\n","        return x\n","\n","# Bloque Transformer: Implementa una capa de Transformer\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n","        super(TransformerBlock, self).__init__()\n","        # Módulo de atención multi-cabeza\n","        self.attention = MultiHeadAttention(d_model, n_heads)\n","        # Normalización de la salida de la atención\n","        self.norm1 = nn.LayerNorm(d_model)\n","        # Módulo feedforward\n","        self.ff = FeedForward(d_model, ff_dim, dropout)\n","        # Normalización de la salida del feedforward\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # Aplicamos atención y normalización\n","        attn_output = self.attention(x)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        # Aplicamos feedforward y normalización\n","        ff_output = self.ff(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x\n","\n","# Modelo Transformer Simple\n","class SimpleTransformer(nn.Module):\n","    def __init__(self, input_dim, d_model, n_heads, ff_dim, n_layers, dropout=0.1):\n","        super(SimpleTransformer, self).__init__()\n","        # Capa de embedding inicial\n","        self.embedding = nn.Linear(input_dim, d_model)\n","        # Múltiples capas Transformer\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n","        ])\n","        # Capa de salida\n","        self.fc_out = nn.Linear(d_model, input_dim)\n","\n","    def forward(self, x):\n","        # Aplicamos el embedding inicial\n","        x = self.embedding(x)\n","        # Pasamos por cada capa Transformer\n","        for layer in self.layers:\n","            x = layer(x)\n","        # Capa de salida final\n","        output = self.fc_out(x)\n","        return output\n","\n","# Definición de hiperparámetros\n","input_dim = 10  # Dimensión de entrada\n","\n","# Parámetros del modelo\n","d_model = 128\n","n_heads = 4\n","ff_dim = 512\n","n_layers = 2\n","\n","# Inicialización del modelo\n","model = SimpleTransformer(input_dim, d_model, n_heads, ff_dim, n_layers)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blhjDnAdhG1r","executionInfo":{"status":"ok","timestamp":1746634540122,"user_tz":-60,"elapsed":29,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"a10c79c3-784f-4f79-f4c6-a8d93dd0396b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleTransformer(\n","  (embedding): Linear(in_features=10, out_features=128, bias=True)\n","  (layers): ModuleList(\n","    (0-1): 2 x TransformerBlock(\n","      (attention): MultiHeadAttention(\n","        (q_linear): Linear(in_features=128, out_features=128, bias=True)\n","        (k_linear): Linear(in_features=128, out_features=128, bias=True)\n","        (v_linear): Linear(in_features=128, out_features=128, bias=True)\n","        (out_linear): Linear(in_features=128, out_features=128, bias=True)\n","      )\n","      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ff): FeedForward(\n","        (fc1): Linear(in_features=128, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=128, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (fc_out): Linear(in_features=128, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["Traductor de idiomas"],"metadata":{"id":"gZodcrhsjUqY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Clase AttentionMechanism: Implementa un mecanismo de atención simple\n","class AttentionMechanism(nn.Module):\n","    def __init__(self, d_model):\n","        super(AttentionMechanism, self).__init__()\n","        # Capas lineales para Query, Key y Value\n","        self.query_layer = nn.Linear(d_model, d_model)\n","        self.key_layer = nn.Linear(d_model, d_model)\n","        self.value_layer = nn.Linear(d_model, d_model)\n","\n","    def forward(self, query, key, value):\n","        # Aplicamos capas lineales para obtener Q, K, V\n","        Q = self.query_layer(query)  # Transformamos el Query\n","        K = self.key_layer(key)      # Transformamos el Key\n","        V = self.value_layer(value)  # Transformamos el Value\n","\n","        # Calculamos los scores de atención\n","        # Multiplicamos Q por la transpuesta de K y escalamos por la raíz cuadrada de la dimensión\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))\n","\n","        # Aplicamos softmax para convertir los scores en probabilidades de atención\n","        attn_weights = F.softmax(scores, dim=-1)\n","\n","        # Multiplicamos los pesos de atención por los valores V\n","        output = torch.matmul(attn_weights, V)\n","\n","        return output, attn_weights\n","\n","# Ejemplo práctico: Traducción de inglés a español\n","# Supongamos que tenemos un diccionario simple con palabras embebidas\n","embedding_dim = 16  # Dimensión del embedding\n","\n","# Secuencia en inglés (5 palabras representadas como vectores de 16 dimensiones)\n","english_sentence = torch.randn((1, 5, embedding_dim))  # (batch_size, seq_len, embedding_dim)\n","\n","# Secuencia en español (7 palabras representadas como vectores de 16 dimensiones)\n","spanish_sentence = torch.randn((1, 7, embedding_dim))  # (batch_size, seq_len, embedding_dim)\n","\n","# Inicializamos el mecanismo de atención\n","attention = AttentionMechanism(embedding_dim)\n","\n","# Aplicamos atención utilizando la oración en inglés como Query y la oración en español como Key/Value\n","output, attn_weights = attention(english_sentence, spanish_sentence, spanish_sentence)\n","\n","# Salida del mecanismo de atención\n","print(\"Output Shape (context vector):\", output.shape)  # (batch_size, seq_len_english, embedding_dim)\n","print(\"Attention Weights Shape:\", attn_weights.shape)  # (batch_size, seq_len_english, seq_len_spanish)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drCpfwkajY2h","executionInfo":{"status":"ok","timestamp":1746635016944,"user_tz":-60,"elapsed":14,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"4bd862d9-eb51-4ac8-fa6b-a5484a8b1c1a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Output Shape (context vector): torch.Size([1, 5, 16])\n","Attention Weights Shape: torch.Size([1, 5, 7])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertTokenizer\n","\n","# Clase TransformerEncoder: Implementa el Encoder del modelo Transformer\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        # Módulo de Auto-Atención Multi-Cabeza\n","        self.self_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n","        # Capa Feed Forward\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, ff_dim),  # Proyección a mayor dimensión\n","            nn.ReLU(),  # Función de activación no lineal\n","            nn.Dropout(dropout),  # Regularización\n","            nn.Linear(ff_dim, d_model)  # Proyección de vuelta a la dimensión original\n","        )\n","        # Capas de Normalización\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # Paso 1: Auto-atención sobre la entrada\n","        attn_output, _ = self.self_attention(x, x, x)\n","        # Residual + Normalización\n","        x = self.norm1(x + attn_output)\n","        # Paso 2: Feedforward\n","        ff_output = self.ff(x)\n","        # Residual + Normalización\n","        x = self.norm2(x + ff_output)\n","        return x\n","\n","# Clase TransformerDecoder: Implementa el Decoder del Transformer\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n","        super(TransformerDecoder, self).__init__()\n","        # Auto-Atención del Decoder\n","        self.self_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n","        # Atención Cruzada con la salida del Encoder\n","        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n","        # Capa Feed Forward\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, ff_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(ff_dim, d_model)\n","        )\n","        # Capas de Normalización\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, enc_output):\n","        # Auto-atención del Decoder\n","        attn_output, _ = self.self_attention(x, x, x)\n","        x = self.norm1(x + attn_output)\n","        # Atención Cruzada con la salida del Encoder\n","        cross_output, _ = self.cross_attention(x, enc_output, enc_output)\n","        x = self.norm2(x + cross_output)\n","        # Feedforward\n","        ff_output = self.ff(x)\n","        x = self.norm3(x + ff_output)\n","        return x\n","\n","# Clase TransformerSummarizer: Implementa el modelo completo para resumen\n","class TransformerSummarizer(nn.Module):\n","    def __init__(self, input_dim, d_model, n_heads, ff_dim, n_layers, dropout=0.1):\n","        super(TransformerSummarizer, self).__init__()\n","        # Capa de Embedding para convertir tokens en vectores\n","        self.embedding = nn.Embedding(input_dim, d_model)\n","        # Lista de capas Encoder\n","        self.encoder_layers = nn.ModuleList([\n","            TransformerEncoder(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n","        ])\n","        # Lista de capas Decoder\n","        self.decoder_layers = nn.ModuleList([\n","            TransformerDecoder(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n","        ])\n","        # Capa de salida para generar las palabras del resumen\n","        self.output_layer = nn.Linear(d_model, input_dim)\n","\n","    def forward(self, src, tgt):\n","        # Embedding de entrada (texto original)\n","        src = self.embedding(src)\n","        # Embedding de la secuencia objetivo (resumen)\n","        tgt = self.embedding(tgt)\n","        # Paso por las capas del Encoder\n","        for layer in self.encoder_layers:\n","            src = layer(src)\n","        # Paso por las capas del Decoder\n","        for layer in self.decoder_layers:\n","            tgt = layer(tgt, src)\n","        # Generación de la secuencia de salida\n","        output = self.output_layer(tgt)\n","        return output\n","\n","# Definición de hiperparámetros\n","input_dim = 30522  # Tamaño del vocabulario BERT (ejemplo)\n","d_model = 256\n","n_heads = 8\n","ff_dim = 512\n","n_layers = 4\n","\n","# Inicialización del modelo con los hiperparámetros definidos\n","model = TransformerSummarizer(input_dim, d_model, n_heads, ff_dim, n_layers)\n","print(model)  # Visualización de la estructura del modelo\n"],"metadata":{"id":"LEsyxOhjmqTS","executionInfo":{"status":"ok","timestamp":1746635770756,"user_tz":-60,"elapsed":3109,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"2c715581-d85b-4b61-c0f4-baa72c356f0e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["TransformerSummarizer(\n","  (embedding): Embedding(30522, 256)\n","  (encoder_layers): ModuleList(\n","    (0-3): 4 x TransformerEncoder(\n","      (self_attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (ff): Sequential(\n","        (0): Linear(in_features=256, out_features=512, bias=True)\n","        (1): ReLU()\n","        (2): Dropout(p=0.1, inplace=False)\n","        (3): Linear(in_features=512, out_features=256, bias=True)\n","      )\n","      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (decoder_layers): ModuleList(\n","    (0-3): 4 x TransformerDecoder(\n","      (self_attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (cross_attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (ff): Sequential(\n","        (0): Linear(in_features=256, out_features=512, bias=True)\n","        (1): ReLU()\n","        (2): Dropout(p=0.1, inplace=False)\n","        (3): Linear(in_features=512, out_features=256, bias=True)\n","      )\n","      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (output_layer): Linear(in_features=256, out_features=30522, bias=True)\n",")\n"]}]}]}