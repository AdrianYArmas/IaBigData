{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6nCdT0aYCWSYgmLR/lwLY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Implementaci칩n Pr치ctica: Transformer desde Cero\n","\n","En este cuaderno, implementaremos los componentes clave de un modelo Transformer siguiendo la arquitectura original \"Attention Is All You Need\". Ejecuta cada celda de c칩digo para definir las clases y funciones necesarias."],"metadata":{"id":"p26sEPFpBiiC"}},{"cell_type":"markdown","source":["### Importaciones y Clases Auxiliares\n","Importamos las librer칤as necesarias y definimos algunas clases fundamentales como la Normalizaci칩n de Capa (LayerNorm) y la Conexi칩n Residual (SublayerConnection)."],"metadata":{"id":"X6n_V0iJB6NQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZghDd6bBTDU","executionInfo":{"status":"ok","timestamp":1746555042373,"user_tz":-60,"elapsed":12816,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"858f23db-49b7-43aa-d0c1-4fae081a7330"},"outputs":[{"output_type":"stream","name":"stdout","text":["Clases LayerNorm y SublayerConnection definidas.\n"]}],"source":["import torch\n","import torch.nn as nn\n","import math\n","import copy # Para clonar m칩dulos\n","\n","class LayerNorm(nn.Module):\n","    # Construye una capa de LayerNorm.\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        # Par치metros aprendibles gamma (a_2) y beta (b_2)\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps # Peque침o valor para evitar divisi칩n por cero\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, seq_len, features)\n","        # Calcula la media y desviaci칩n est치ndar sobre la 칰ltima dimensi칩n (features)\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        # Aplica la normalizaci칩n\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n","\n","class SublayerConnection(nn.Module):\n","    # Conexi칩n residual seguida de LayerNorm. Nota: La normalizaci칩n va ANTES de la subcapa aqu칤.\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        # Aplica la conexi칩n residual a cualquier subcapa del mismo tama침o.\n","        # Normaliza x, pasa por la subcapa (con dropout), y a침ade la entrada original x (conexi칩n residual)\n","        return x + self.dropout(sublayer(self.norm(x)))\n","\n","print(\"Clases LayerNorm y SublayerConnection definidas.\")\n"]},{"cell_type":"markdown","source":["### Atenci칩n Multi-Cabeza (Multi-Head Attention)"],"metadata":{"id":"QfHxkUKkCOLu"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    # Implementaci칩n de la atenci칩n multi-cabeza.\n","    def __init__(self, h, d_model, dropout=0.1):\n","        # Toma el n칰mero de cabezas (h) y la dimensi칩n del modelo (d_model).\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % h == 0\n","        # Asumimos que d_v siempre es igual a d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        # Creamos 4 capas lineales: para Q, K, V y la salida final\n","        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n","        self.attn = None # Para almacenar los pesos de atenci칩n para visualizaci칩n/an치lisis\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def attention(self, query, key, value, mask=None, dropout=None):\n","        # Calcula la salida de la atenci칩n escalada por producto punto.\n","        d_k = query.size(-1)\n","        # 1. Calcula scores: (Batch, h, Seq_q, d_k) @ (Batch, h, d_k, Seq_k) -> (Batch, h, Seq_q, Seq_k)\n","        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n","        # 2. Aplica m치scara (si existe)\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9) # Rellena con valor muy negativo donde mask es 0\n","        # 3. Aplica softmax para obtener pesos\n","        p_attn = torch.softmax(scores, dim=-1)\n","        if dropout is not None:\n","            p_attn = dropout(p_attn)\n","        # 4. Multiplica pesos por Values: (Batch, h, Seq_q, Seq_k) @ (Batch, h, Seq_k, d_k) -> (Batch, h, Seq_q, d_k)\n","        return torch.matmul(p_attn, value), p_attn\n","\n","    def forward(self, query, key, value, mask=None):\n","        # Implementa la figura 2 del paper.\n","        if mask is not None:\n","            # Aplica la misma m치scara a todas las cabezas\n","            mask = mask.unsqueeze(1) # (Batch, 1, Seq_q, Seq_k) o (Batch, 1, 1, Seq_k)\n","        nbatches = query.size(0)\n","\n","        # 1) Hacer proyecci칩n lineal en batch de d_model => h x d_k\n","        # query, key, value: (Batch, Seq, d_model)\n","        # -> aplica linear y view -> (Batch, Seq, h, d_k)\n","        # -> transpose -> (Batch, h, Seq, d_k)\n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","\n","        # 2) Aplicar atenci칩n en todos los vectores proyectados en batch.\n","        # x: (Batch, h, Seq_q, d_k)\n","        # self.attn: (Batch, h, Seq_q, Seq_k)\n","        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n","\n","        # 3) \"Concatenar\" usando view y aplicar proyecci칩n lineal final.\n","        # x: (Batch, h, Seq_q, d_k) -> transpose -> (Batch, Seq_q, h, d_k)\n","        # -> contiguous + view -> (Batch, Seq_q, d_model)\n","        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n","        # Aplica la capa lineal final W_o\n","        return self.linears[-1](x)\n","\n","print(\"Clase MultiHeadAttention definida.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKiGcverCR0e","executionInfo":{"status":"ok","timestamp":1746555293853,"user_tz":-60,"elapsed":4,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"bc42ed6d-66d3-401b-e73a-9d1c845334bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase MultiHeadAttention definida.\n"]}]},{"cell_type":"markdown","source":["###Red Feed-Forward (Position-wise)\n","Cada capa del Transformer tambi칠n contiene una red feed-forward simple que se aplica independientemente a cada posici칩n."],"metadata":{"id":"Zs3omJzkCZLv"}},{"cell_type":"code","source":["class PositionwiseFeedForward(nn.Module):\n","    # Implementa una red FFN.\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff) # Capa de expansi칩n\n","        self.w_2 = nn.Linear(d_ff, d_model) # Capa de contracci칩n\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU() # Activaci칩n ReLU (com칰n)\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq, d_model)\n","        # -> w_1 -> (Batch, Seq, d_ff)\n","        # -> activation -> (Batch, Seq, d_ff)\n","        # -> dropout -> (Batch, Seq, d_ff)\n","        # -> w_2 -> (Batch, Seq, d_model)\n","        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n","\n","print(\"Clase PositionwiseFeedForward definida.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OOeJy7bOCdkt","executionInfo":{"status":"ok","timestamp":1746555690489,"user_tz":-60,"elapsed":15,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"f988d588-2c1c-4893-b43e-812bacc75a90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clase PositionwiseFeedForward definida.\n"]}]},{"cell_type":"markdown","source":["### Embeddings y Codificaci칩n Posicional\n","Convertimos los tokens de entrada en vectores (embeddings) y a침adimos informaci칩n sobre su posici칩n en la secuencia."],"metadata":{"id":"kRPXdDajCiC1"}},{"cell_type":"code","source":["class Embeddings(nn.Module):\n","    def __init__(self, d_model, vocab):\n","        # vocab: tama침o del vocabulario.\n","        super(Embeddings, self).__init__()\n","        self.lut = nn.Embedding(vocab, d_model) # Capa de embedding\n","        self.d_model = d_model\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq) - 칈ndices de tokens\n","        # -> lut -> (Batch, Seq, d_model)\n","        # Multiplica por sqrt(d_model) seg칰n el paper\n","        return self.lut(x) * math.sqrt(self.d_model)\n","\n","class PositionalEncoding(nn.Module):\n","    # Implementa la codificaci칩n posicional PE.\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        # max_len: longitud m치xima de secuencia esperada.\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Calcula los valores de codificaci칩n posicional una vez en log space\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n","        # T칠rmino de divisi칩n para las frecuencias\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        # Calcula seno para posiciones pares y coseno para impares\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0) # A침adir dimensi칩n de batch: (1, max_len, d_model)\n","        # Registrar 'pe' como buffer, no como par치metro entrenable\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq, d_model)\n","        # A침ade la codificaci칩n posicional a los embeddings\n","        # Necesitamos tomar solo las primeras 'Seq' codificaciones posicionales\n","        # self.pe[:, :x.size(1)] -> (1, Seq, d_model)\n","        x = x + self.pe[:, :x.size(1)].requires_grad_(False) # No requiere gradiente\n","        return self.dropout(x)\n","\n","print(\"Clases Embeddings y PositionalEncoding definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A2r41Qp_CmFN","executionInfo":{"status":"ok","timestamp":1746555695877,"user_tz":-60,"elapsed":6,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"e60a69bb-bcf4-414a-cbcd-e4c82aee0f11"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases Embeddings y PositionalEncoding definidas.\n"]}]},{"cell_type":"markdown","source":["### Capas del Codificador y Decodificador\n","Ahora ensamblamos las subcapas (atenci칩n y feed-forward) para formar las capas del codificador y decodificador."],"metadata":{"id":"p4u0zFLXC6Gk"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    # El Encoder se compone de auto-atenci칩n y feed-forward.\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        # size: d_model.\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn # Subcapa de auto-atenci칩n\n","        self.feed_forward = feed_forward # Subcapa feed-forward\n","        # Dos conexiones residuales + LayerNorm\n","        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(2)])\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        # Sigue la conexi칩n definida en SublayerConnection.\n","        # 1. Auto-atenci칩n (Query, Key, Value son iguales a x)\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        # 2. Feed-forward\n","        return self.sublayer[1](x, self.feed_forward)\n","\n","class DecoderLayer(nn.Module):\n","    # El Decoder se compone de auto-atenci칩n, atenci칩n fuente-objetivo y feed-forward.\n","    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n","        super(DecoderLayer, self).__init__()\n","        self.size = size\n","        self.self_attn = self_attn # Auto-atenci칩n (enmascarada) sobre la salida del decoder\n","        self.src_attn = src_attn   # Atenci칩n sobre la salida del encoder (memoria)\n","        self.feed_forward = feed_forward\n","        # Tres conexiones residuales + LayerNorm\n","        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(3)])\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        # x: entrada del decoder (salida previa + pos encoding)\n","        # memory: salida del encoder\n","        # src_mask: m치scara para el padding en la secuencia de entrada (encoder)\n","        # tgt_mask: m치scara para el padding y para evitar atenci칩n futura en la secuencia de salida (decoder)\n","\n","        m = memory\n","        # 1. Auto-atenci칩n enmascarada (Query, Key, Value son iguales a x)\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n","        # 2. Atenci칩n sobre la salida del encoder (Query=x, Key=memory, Value=memory)\n","        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n","        # 3. Capa FeedForward\n","        return self.sublayer[2](x, self.feed_forward)\n","\n","print(\"Clases EncoderLayer y DecoderLayer definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROOkyi-BC_WV","executionInfo":{"status":"ok","timestamp":1746555913371,"user_tz":-60,"elapsed":52,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"95934d4e-59d8-42a6-9ac9-79f397fe6fac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases EncoderLayer y DecoderLayer definidas.\n"]}]},{"cell_type":"markdown","source":["### Ensamblaje del Codificador y Decodificador\n","Apilamos m칰ltiples capas para formar el codificador y el decodificador completos."],"metadata":{"id":"819Lxo1tDB_V"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    # Encoder central con N capas.\n","    def __init__(self, layer, N):\n","        # layer: una instancia de EncoderLayer. N: n칰mero de capas.\n","        super(Encoder, self).__init__()\n","        # Clona la capa N veces\n","        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n","        # Normalizaci칩n final despu칠s de la 칰ltima capa\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, mask):\n","        # Pasa la entrada (y m치scara) a trav칠s de cada capa.\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n","class Decoder(nn.Module):\n","    # Decoder gen칠rico con N capas.\n","    def __init__(self, layer, N):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n","        self.norm = LayerNorm(layer.size)\n","\n","    def forward(self, x, memory, src_mask, tgt_mask):\n","        # Pasa la entrada (y m치scaras) a trav칠s de cada capa.\n","        for layer in self.layers:\n","            x = layer(x, memory, src_mask, tgt_mask)\n","        return self.norm(x)\n","\n","print(\"Clases Encoder y Decoder definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jzx_vCvDLyO","executionInfo":{"status":"ok","timestamp":1746555928156,"user_tz":-60,"elapsed":48,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"20e8426e-d332-47f6-ae1c-b2dfbe4ab0b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases Encoder y Decoder definidas.\n"]}]},{"cell_type":"markdown","source":["### Modelo Transformer Completo y Generador\n","Finalmente, unimos el codificador, el decodificador y las capas de embedding, junto con una capa final (Generador) para producir las probabilidades de salida sobre el vocabulario."],"metadata":{"id":"lKL-np4_Db18"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","    # Define la capa de generaci칩n lineal est치ndar + softmax.\n","    def __init__(self, d_model, vocab):\n","        # vocab: tama침o del vocabulario de salida.\n","        super(Generator, self).__init__()\n","        # Proyecta de d_model a la dimensi칩n del vocabulario\n","        self.proj = nn.Linear(d_model, vocab)\n","\n","    def forward(self, x):\n","        # x: (Batch, Seq, d_model)\n","        # -> proj -> (Batch, Seq, vocab)\n","        # Aplicar log_softmax es com칰n para usar con NLLLoss durante el entrenamiento\n","        return torch.log_softmax(self.proj(x), dim=-1)\n","\n","class Transformer(nn.Module):\n","    # Implementaci칩n del modelo Transformer completo.\n","    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n","        super(Transformer, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.src_embed = src_embed # Embedding + Positional Encoding para la entrada\n","        self.tgt_embed = tgt_embed # Embedding + Positional Encoding para la salida\n","        self.generator = generator # Capa lineal final + Softmax\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        # Procesa secuencias de entrada y salida enmascaradas.\n","        # 1. Pasa la entrada por el codificador\n","        memory = self.encode(src, src_mask)\n","        # 2. Pasa la salida del codificador y la entrada del decodificador por el decodificador\n","        return self.decode(memory, src_mask, tgt, tgt_mask)\n","\n","    def encode(self, src, src_mask):\n","        # Aplica embedding + pos encoding a la entrada, luego pasa por el codificador\n","        return self.encoder(self.src_embed(src), src_mask)\n","\n","    def decode(self, memory, src_mask, tgt, tgt_mask):\n","        # Aplica embedding + pos encoding a la entrada del decodificador, luego pasa por el decodificador\n","        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n","\n","print(\"Clases Generator y Transformer definidas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmQp4ww6DhxM","executionInfo":{"status":"ok","timestamp":1746556075468,"user_tz":-60,"elapsed":18,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"136fa756-9601-4ee1-efbb-adcb0a03c75e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Clases Generator y Transformer definidas.\n"]}]},{"cell_type":"markdown","source":["###Funci칩n Constructora y Ejemplo de Uso\n","Creamos una funci칩n para construir el modelo con hiperpar치metros espec칤ficos y probamos pasar datos ficticios a trav칠s de 칠l."],"metadata":{"id":"isAEHzWHDjz5"}},{"cell_type":"code","source":["def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    # Construye un modelo Transformer completo con hiperpar치metros.\n","    c = copy.deepcopy\n","    attn = MultiHeadAttention(h, d_model, dropout)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout)\n","\n","    # Crear el modelo completo\n","    model = Transformer(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n","        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n","        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n","        Generator(d_model, tgt_vocab)\n","    )\n","\n","    # Inicializar par치metros con Xavier Glorot (importante para convergencia)\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","\n","    return model\n","\n","# --- Ejemplo de uso --- #\n","\n","# Par치metros de ejemplo (puedes cambiarlos)\n","SRC_VOCAB_SIZE = 1000 # Tama침o vocabulario fuente peque침o para ejemplo\n","TGT_VOCAB_SIZE = 1200 # Tama침o vocabulario objetivo peque침o para ejemplo\n","N_LAYERS = 2        # N칰mero de capas (reducido para rapidez)\n","D_MODEL = 128       # Dimensi칩n del modelo (reducido)\n","D_FF = 256          # Dimensi칩n FeedForward (reducido)\n","NUM_HEADS = 4         # N칰mero de cabezas (reducido)\n","DROPOUT = 0.1\n","\n","# Crear modelo\n","model = make_model(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, N=N_LAYERS, d_model=D_MODEL, d_ff=D_FF, h=NUM_HEADS, dropout=DROPOUT)\n","print(f\"Modelo Transformer creado con {N_LAYERS} capas, d_model={D_MODEL}, {NUM_HEADS} cabezas.\")\n","# print(model) # Descomenta para ver la estructura detallada\n","\n","# Ejemplo de datos dummy (Batch=2, Seq_len_src=10, Seq_len_tgt=8)\n","# Tokens de entrada (valores entre 1 y SRC_VOCAB_SIZE-1, 0 es padding)\n","src_dummy = torch.randint(1, SRC_VOCAB_SIZE, (2, 10))\n","src_dummy[0, 7:] = 0 # A침adir padding a la primera secuencia\n","# Tokens de salida esperados (para entrenamiento teacher forcing)\n","# Valores entre 1 y TGT_VOCAB_SIZE-1, 0 es padding\n","tgt_dummy = torch.randint(1, TGT_VOCAB_SIZE, (2, 8))\n","tgt_dummy[1, 6:] = 0 # A침adir padding a la segunda secuencia\n","# La entrada real al decoder durante el entrenamiento es tgt excepto el 칰ltimo token\n","tgt_dummy_input = tgt_dummy[:, :-1]\n","\n","# Crear m치scaras\n","# M치scara de padding para src: (Batch, 1, Seq_len) - True donde NO es padding\n","src_mask_dummy = (src_dummy != 0).unsqueeze(1)\n","# M치scara de padding para tgt_input: (Batch, 1, Seq_len)\n","_tgt_mask_padding = (tgt_dummy_input != 0).unsqueeze(1)\n","# M치scara futura para tgt_input (triangular inferior): (1, Seq_len, Seq_len)\n","def subsequent_mask(size):\n","    \"Enmascara posiciones futuras.\"\n","    attn_shape = (1, size, size)\n","    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n","    return subsequent_mask == 0 # Devuelve True donde NO est치 enmascarado\n","\n","_tgt_mask_future = subsequent_mask(tgt_dummy_input.size(-1))\n","# Combinar m치scara de padding y futura para el decoder\n","tgt_mask_dummy = _tgt_mask_padding & _tgt_mask_future\n","\n","# Pasar datos por el modelo (solo forward pass, sin entrenamiento)\n","# Aseg칰rate de que el modelo est칠 en modo evaluaci칩n si no est치s entrenando\n","model.eval()\n","with torch.no_grad(): # No calcular gradientes para este ejemplo\n","    output = model(src_dummy, tgt_dummy_input, src_mask_dummy, tgt_mask_dummy)\n","    # Pasar la salida por el generador para obtener log probabilidades\n","    final_output_log_probs = model.generator(output)\n","\n","print(f\"\\nForma de la entrada src: {src_dummy.shape}\")\n","print(f\"Forma de la entrada tgt (input decoder): {tgt_dummy_input.shape}\")\n","print(f\"Forma de la m치scara src: {src_mask_dummy.shape}\")\n","print(f\"Forma de la m치scara tgt: {tgt_mask_dummy.shape}\")\n","print(f\"Forma de la salida del decoder (antes del generador): {output.shape}\") # Esperado: (Batch, Seq_len_tgt-1, d_model)\n","print(f\"Forma de la salida final (log probs): {final_output_log_probs.shape}\") # Esperado: (Batch, Seq_len_tgt-1, tgt_vocab_size)\n","\n","# Puedes inspeccionar los valores si quieres\n","# print(\"\\nLog probabilidades del primer token de la primera secuencia:\")\n","# print(final_output_log_probs[0, 0, :])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Po32iDHWDwNK","executionInfo":{"status":"ok","timestamp":1746556132423,"user_tz":-60,"elapsed":49,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"da0009f2-8258-4dfb-8386-54ed2a3e46bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo Transformer creado con 2 capas, d_model=128, 4 cabezas.\n","\n","Forma de la entrada src: torch.Size([2, 10])\n","Forma de la entrada tgt (input decoder): torch.Size([2, 7])\n","Forma de la m치scara src: torch.Size([2, 1, 10])\n","Forma de la m치scara tgt: torch.Size([2, 7, 7])\n","Forma de la salida del decoder (antes del generador): torch.Size([2, 7, 128])\n","Forma de la salida final (log probs): torch.Size([2, 7, 1200])\n"]}]},{"cell_type":"markdown","source":["**Explicaci칩n Pr치ctica:**\n","\n","*   Hemos definido todas las clases necesarias para un Transformer.\n","*   La funci칩n `make_model` facilita la creaci칩n del modelo con diferentes hiperpar치metros.\n","*   El ejemplo de uso muestra c칩mo crear el modelo, preparar datos ficticios (incluyendo padding) y las m치scaras correspondientes (padding y futura).\n","*   Finalmente, pasamos los datos por el modelo para obtener las log-probabilidades de salida. En un escenario real, estas se usar칤an con una funci칩n de p칠rdida (como `NLLLoss`) para entrenar el modelo.\n","*   **춰Experimenta!** Cambia los hiperpar치metros (`N_LAYERS`, `D_MODEL`, `NUM_HEADS`), los tama침os de secuencia o batch y observa c칩mo afecta (aunque aqu칤 solo hacemos un forward pass).\n","\n","---\n"],"metadata":{"id":"mRnfqRFvD_Tj"}},{"cell_type":"markdown","source":["## Comparaci칩n Pr치ctica: Transformers vs. RNNs/CNNs\n","\n","Antes de los Transformers, las Redes Neuronales Recurrentes (RNNs, como LSTMs y GRUs) y las Redes Neuronales Convolucionales (CNNs) eran las arquitecturas dominantes en NLP.\n","\n","*   **RNNs:** Procesan secuencias token por token, manteniendo un estado oculto que (idealmente) captura informaci칩n de tokens anteriores. Son inherentemente secuenciales.\n","*   **CNNs:** Aplican filtros (convoluciones) sobre ventanas de tokens, capturando patrones locales. Se pueden apilar para capturar contextos m치s amplios, pero no son tan naturales para dependencias a largo plazo como las RNNs.\n","\n","**Ventajas Pr치cticas de los Transformers:**\n","\n","1.  **Paralelizaci칩n Superior:** A diferencia del procesamiento secuencial de las RNNs, los c치lculos dentro de un Transformer (especialmente la auto-atenci칩n y las capas feed-forward) pueden paralelizarse masivamente a lo largo de la dimensi칩n de la secuencia. Esto permite entrenar modelos mucho m치s grandes en hardware moderno (GPUs/TPUs) de manera significativamente m치s r치pida que RNNs equivalentes en tama침o.\n","2.  **Mejor Captura de Dependencias a Largo Plazo:** La auto-atenci칩n calcula directamente las interacciones entre cualquier par de tokens en la secuencia, sin importar su distancia. Esto supera la dificultad de las RNNs para propagar informaci칩n a trav칠s de muchos pasos de tiempo (problema del desvanecimiento del gradiente), permitiendo a los Transformers modelar relaciones complejas en textos largos de manera m치s efectiva.\n","3.  **Representaciones Contextuales Ricas:** La auto-atenci칩n permite que la representaci칩n de cada token se base en una combinaci칩n ponderada de todos los dem치s tokens, generando embeddings contextuales muy potentes (ej. BERT).\n","\n","**Desventajas Pr치cticas de los Transformers (Est치ndar):**\n","\n","1.  **Complejidad Cuadr치tica con la Longitud de Secuencia:** La auto-atenci칩n est치ndar requiere calcular una puntuaci칩n para cada par de tokens, lo que resulta en una complejidad computacional y de memoria de O(n^2), donde n es la longitud de la secuencia. Esto hace que procesar secuencias muy largas (miles de tokens) sea computacionalmente muy costoso o inviable con la arquitectura original. Las RNNs tienen complejidad lineal O(n).\n","2.  **Mayor Necesidad de Datos y C칩mputo (para modelos grandes):** Aunque pueden entrenarse m치s r치pido debido a la paralelizaci칩n, los modelos Transformer m치s potentes (como BERT, GPT) suelen ser muy grandes y requieren enormes cantidades de datos y recursos computacionales para su pre-entrenamiento.\n","3.  **Menos Inducci칩n Secuencial Inherente:** No tienen un sesgo inductivo tan fuerte hacia el orden secuencial como las RNNs. La informaci칩n posicional debe a침adirse expl칤citamente (Codificaci칩n Posicional).\n","\n","**En resumen:** Para la mayor칤a de las tareas de NLP modernas, las ventajas de paralelizaci칩n y captura de contexto de los Transformers superan sus desventajas, especialmente con la disponibilidad de modelos pre-entrenados y arquitecturas eficientes para secuencias largas.\n","\n","---"],"metadata":{"id":"oH0y7e74Eh3J"}},{"cell_type":"markdown","source":["## 3. Atenci칩n Pr치ctica: Queries, Keys y Values\n","\n","El mecanismo central es la **Atenci칩n Escalada por Producto Punto (Scaled Dot-Product Attention)**. Funciona con tres vectores derivados de la entrada para cada token: Query (Q), Key (K) y Value (V).\n","\n","*   **Query (Q):** Representa el token actual que est치 \"preguntando\" o buscando informaci칩n relevante.\n","*   **Key (K):** Representa un token en la secuencia (potencialmente todos, incluida ella misma) que \"anuncia\" la informaci칩n que posee.\n","*   **Value (V):** Representa el contenido o la informaci칩n real del token asociado a la Key.\n","\n","**Flujo de C치lculo:**\n","\n","1.  **Scores:** Calcula qu칠 tan relevante es cada Key para una Query dada. Se hace mediante el producto punto: `Score = Q * K^T`.\n","2.  **Escalado:** Divide los scores por la ra칤z cuadrada de la dimensi칩n de los vectores Key (`d_k`) para estabilizar los gradientes: `Scaled_Score = Score / sqrt(d_k)`.\n","3.  **M치scara (Opcional):** Si se proporciona una m치scara (ej. para ignorar padding o posiciones futuras), los scores correspondientes se establecen en un valor muy negativo (ej. -infinito) antes del softmax.\n","4.  **Ponderaci칩n (Softmax):** Aplica softmax a los scores escalados (y enmascarados) para obtener pesos de atenci칩n que suman 1. Estos pesos indican cu치nta atenci칩n debe prestar la Query a cada Value: `Weights = softmax(Scaled_Score)`.\n","5.  **Salida:** Calcula la suma ponderada de los Values usando los pesos de atenci칩n: `Output = Weights * V`.\n"],"metadata":{"id":"7gP-a2OJEtxh"}},{"cell_type":"markdown","source":["Ejemplo Num칠rico Simple\n","\n","Veamos un ejemplo muy simplificado con un solo Query, tres Keys/Values y una dimensi칩n `d_k` peque침a."],"metadata":{"id":"bL0IeVSzEznA"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","import math\n","\n","# Dimensiones simplificadas\n","d_k = 4 # Dimensi칩n de Key/Query/Value\n","seq_len_k = 3 # N칰mero de Keys/Values\n","\n","# Vectores de ejemplo (Batch=1, Num_Heads=1)\n","# Suponemos que ya han pasado por las proyecciones lineales\n","query = torch.randn(1, 1, d_k) # (Batch, Seq_q=1, d_k)\n","keys = torch.randn(1, seq_len_k, d_k) # (Batch, Seq_k, d_k)\n","values = torch.randn(1, seq_len_k, d_k) # (Batch, Seq_k, d_k)\n","\n","print(f\"Query (Q) shape: {query.shape}\")\n","print(f\"Keys (K) shape: {keys.shape}\")\n","print(f\"Values (V) shape: {values.shape}\")\n","\n","# 1. Calcular Scores (Producto Punto)\n","# query: (1, 1, d_k) -> (1, d_k)\n","# keys.transpose(-2, -1): (1, d_k, Seq_k)\n","# scores: (1, 1, d_k) @ (1, d_k, Seq_k) -> (1, 1, Seq_k)\n","scores = torch.matmul(query, keys.transpose(-2, -1))\n","print(f\"\\nScores (Q @ K^T) shape: {scores.shape}\")\n","print(f\"Scores: {scores}\")\n","\n","# 2. Escalar\n","scaled_scores = scores / math.sqrt(d_k)\n","print(f\"\\nScaled Scores shape: {scaled_scores.shape}\")\n","print(f\"Scaled Scores: {scaled_scores}\")\n","\n","# 3. M치scara (Opcional) - Ejemplo: Ignorar el 칰ltimo Key/Value\n","# mask shape debe ser compatible para broadcast: (Batch, Seq_q, Seq_k)\n","mask = torch.tensor([[[True, True, False]]]) # (1, 1, 3) - True donde NO est치 enmascarado\n","print(f\"\\nMask: {mask}\")\n","scaled_scores_masked = scaled_scores.masked_fill(mask == 0, -1e9)\n","print(f\"Scaled Scores (Masked): {scaled_scores_masked}\")\n","\n","# 4. Ponderaci칩n (Softmax)\n","# Aplicamos softmax sobre la 칰ltima dimensi칩n (Seq_k)\n","attention_weights = F.softmax(scaled_scores_masked, dim=-1)\n","print(f\"\\nAttention Weights (Softmax) shape: {attention_weights.shape}\")\n","print(f\"Attention Weights: {attention_weights}\")\n","# Nota: El peso para la posici칩n enmascarada (la 칰ltima) deber칤a ser cercano a cero.\n","\n","# 5. Salida (Suma Ponderada de Values)\n","# attention_weights: (1, 1, Seq_k)\n","# values: (1, Seq_k, d_k)\n","# output: (1, 1, Seq_k) @ (1, Seq_k, d_k) -> (1, 1, d_k)\n","output = torch.matmul(attention_weights, values)\n","print(f\"\\nOutput (Weighted Sum of V) shape: {output.shape}\")\n","print(f\"Output: {output}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3GipmouWFFv_","executionInfo":{"status":"ok","timestamp":1746557898940,"user_tz":-60,"elapsed":53,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"1b5b785c-6600-4a54-c2fc-0e65f63c5a7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Query (Q) shape: torch.Size([1, 1, 4])\n","Keys (K) shape: torch.Size([1, 3, 4])\n","Values (V) shape: torch.Size([1, 3, 4])\n","\n","Scores (Q @ K^T) shape: torch.Size([1, 1, 3])\n","Scores: tensor([[[-0.6446, -0.3494,  1.9690]]])\n","\n","Scaled Scores shape: torch.Size([1, 1, 3])\n","Scaled Scores: tensor([[[-0.3223, -0.1747,  0.9845]]])\n","\n","Mask: tensor([[[ True,  True, False]]])\n","Scaled Scores (Masked): tensor([[[-3.2232e-01, -1.7471e-01, -1.0000e+09]]])\n","\n","Attention Weights (Softmax) shape: torch.Size([1, 1, 3])\n","Attention Weights: tensor([[[0.4632, 0.5368, 0.0000]]])\n","\n","Output (Weighted Sum of V) shape: torch.Size([1, 1, 4])\n","Output: tensor([[[-0.6992, -0.0519,  0.6605,  1.1972]]])\n"]}]},{"cell_type":"markdown","source":["**Explicaci칩n Pr치ctica:**\n","\n","*   El c칩digo sigue los 5 pasos descritos.\n","*   La `query` calcula su similitud (`scores`) con cada `key`.\n","*   Los `scores` se escalan y opcionalmente se enmascaran.\n","*   `softmax` convierte los scores en `attention_weights` (una distribuci칩n de probabilidad).\n","*   La `output` es una combinaci칩n de los `values`, ponderada por cu치nta atenci칩n recibi칩 cada uno. El `value` correspondiente a la `key` enmascarada contribuye muy poco o nada a la salida.\n","*   En la **Atenci칩n Multi-Cabeza**, este proceso se realiza en paralelo con diferentes proyecciones de Q, K, V (las \"cabezas\"), y los resultados se concatenan y proyectan linealmente al final.\n","\n","---"],"metadata":{"id":"SlcFZT_5FMD_"}},{"cell_type":"markdown","source":["## Modelos Preentrenados y Ajuste Fino (Fine-Tuning) con 游뱅 Transformers\n","\n","Entrenar un Transformer desde cero requiere muchos datos y recursos. En la pr치ctica, es mucho m치s com칰n usar **modelos pre-entrenados** y **ajustarlos (fine-tuning)** para una tarea espec칤fica. La librer칤a `transformers` de Hugging Face 游뱅 facilita enormemente este proceso.\n","\n","**Conceptos Clave:**\n","\n","*   **Pre-entrenamiento:** Modelos enormes (como BERT, GPT, T5) se entrenan en corpus masivos de texto no etiquetado (ej. Wikipedia, libros, web) con objetivos como predecir palabras enmascaradas (BERT) o predecir la siguiente palabra (GPT). Aprenden representaciones ling칲칤sticas generales.\n","*   **Ajuste Fino (Fine-Tuning):** Se toma un modelo pre-entrenado y se contin칰a entrenando (generalmente solo las 칰ltimas capas o todas con una tasa de aprendizaje baja) en un conjunto de datos m치s peque침o y espec칤fico de la tarea (ej. clasificaci칩n de sentimientos, respuesta a preguntas).\n","\n","**Modelos Populares:**\n","\n","*   **BERT (Bidirectional Encoder Representations from Transformers):** Basado en el codificador. Excelente para tareas de comprensi칩n (clasificaci칩n, NER, Q&A). Aprende contexto de izquierda a derecha y de derecha a izquierda.\n","*   **GPT (Generative Pre-trained Transformer):** Basado en el decodificador. Excelente para generaci칩n de texto (escritura creativa, chatbots). Es autorregresivo (predice el siguiente token basado en los anteriores).\n","*   **T5 (Text-to-Text Transfer Transformer):** Arquitectura Encoder-Decoder. Trata todas las tareas como \"texto a texto\" a침adiendo prefijos (ej. \"summarize: ...\", \"translate English to German: ...\"). Muy vers치til.\n","*   **DistilBERT:** Una versi칩n m치s peque침a y r치pida de BERT, creada mediante destilaci칩n del conocimiento. Ideal para entornos con recursos limitados.\n"],"metadata":{"id":"nE8dszjKFqht"}},{"cell_type":"markdown","source":["### Instalaci칩n de Librer칤as\n","\n","Necesitamos instalar `transformers`, `datasets` (para cargar/manejar datos f치cilmente) y `evaluate` (para m칠tricas)."],"metadata":{"id":"3J79BYTHF9de"}},{"cell_type":"code","source":["# 춰Aseg칰rate de ejecutar esta celda!\n","!pip install transformers datasets evaluate torch accelerate -q\n","!pip install numpy==1.23.5 -q\n","\n","\n","print(\"Librer칤as instaladas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFEsf5DOGDV3","executionInfo":{"status":"ok","timestamp":1746559311670,"user_tz":-60,"elapsed":14954,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"034900b1-36de-479b-d016-f8a16da62f06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Librer칤as instaladas.\n"]}]},{"cell_type":"markdown","source":["Ejemplo Pr치ctico: Fine-Tuning para Clasificaci칩n de Texto\n","\n","Vamos a ajustar DistilBERT (una versi칩n m치s ligera de BERT) para una tarea de clasificaci칩n de sentimientos usando un peque침o dataset de ejemplo.\n","\n","**1. Cargar Dataset y Tokenizador**\n","\n","Usaremos un dataset de ejemplo simple. En una aplicaci칩n real, usar칤as `datasets.load_dataset(\"nombre_del_dataset\")`."],"metadata":{"id":"s3Wsh0D5GNNH"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from datasets import Dataset\n","import torch\n","import numpy as np\n","import evaluate\n","\n","# Modelo pre-entrenado a usar (ligero y r치pido)\n","model_name = \"distilbert-base-uncased\"\n","\n","# Cargar tokenizador asociado al modelo\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Datos de ejemplo (muy peque침os para demostraci칩n)\n","texts = [\"I love this movie, it's fantastic!\",\n","         \"This was the worst film I have ever seen.\",\n","         \"The acting was okay, but the plot was boring.\",\n","         \"Absolutely brilliant, a must-watch!\",\n","         \"I didn't like it very much.\",\n","         \"A masterpiece of cinema.\"]\n","labels = [1, 0, 0, 1, 0, 1] # 1: Positivo, 0: Negativo\n","\n","# Crear un Dataset de Hugging Face\n","data = {\"text\": texts, \"label\": labels}\n","dataset = Dataset.from_dict(data)\n","\n","# Dividir en entrenamiento y evaluaci칩n (simple split para ejemplo)\n","dataset = dataset.train_test_split(test_size=0.3) # 70% train, 30% test\n","\n","print(\"Dataset de ejemplo:\")\n","print(dataset)\n","\n","# Funci칩n para tokenizar los textos\n","def tokenize_function(examples):\n","    # padding='max_length' asegura que todas las secuencias tengan la misma longitud\n","    # truncation=True corta secuencias m치s largas que max_length\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n","\n","# Aplicar tokenizaci칩n a todo el dataset\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","\n","# Eliminar la columna de texto original, ya no la necesitamos\n","tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n","# Renombrar 'label' a 'labels' (esperado por el Trainer)\n","tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n","# Establecer el formato a tensores de PyTorch\n","tokenized_datasets.set_format(\"torch\")\n","\n","print(\"\\nDataset tokenizado:\")\n","print(tokenized_datasets)\n","print(\"\\nEjemplo de entrada tokenizada:\")\n","# Accessing the first element without slicing\n","print(tokenized_datasets[\"train\"][0])\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"6-q5bCjUGSkk","executionInfo":{"status":"error","timestamp":1746559341975,"user_tz":-60,"elapsed":25992,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"d556f78a-a08d-40cd-a480-8e32ee7f19a3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m )\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLOSS_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m from .pytorch_utils import (  # noqa: F401\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_deformable_detr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeformableDetrForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeformableDetrForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss_for_object_detection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mForObjectDetectionLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForSegmentationLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/loss/loss_deformable_detr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcenter_to_corners_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scipy_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_transforms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpsSet\u001b[0m \u001b[0;31m# line: 170\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauthoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyzer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelAnalyzer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAnalyzer\u001b[0m \u001b[0;31m# line: 35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/lite/experimental/authoring/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthoring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatible\u001b[0m \u001b[0;31m# line: 263\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/authoring/authoring.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlite_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_phase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from jax._src.core import (\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0mAbstractToken\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mAbstractToken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/dtypes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0m_string_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'StringDType'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mxla_extension_version\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m311\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m   \u001b[0m_string_types\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mJAXType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringDType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0;32mdel\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'dtypes'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e9b30d1bc53b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1967\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1969\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1970\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_utils because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'dtypes'"]}]},{"cell_type":"markdown","source":["**2. Cargar Modelo Pre-entrenado**\n","\n","Cargamos DistilBERT pre-entrenado, pero especificamos que es para clasificaci칩n de secuencias y el n칰mero de etiquetas (2 en nuestro caso: positivo/negativo).\n"],"metadata":{"id":"SpTNLnVRG6Zu"}},{"cell_type":"code","source":["# Cargar el modelo pre-entrenado para clasificaci칩n de secuencias\n","# num_labels indica cu치ntas clases de salida hay\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","\n","print(f\"Modelo {model_name} cargado para clasificaci칩n con {model.config.num_labels} etiquetas.\")\n"],"metadata":{"id":"dPku1k9vG88V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. Definir M칠tricas y Argumentos de Entrenamiento**\n","\n","Necesitamos una funci칩n para calcular m칠tricas durante la evaluaci칩n y definir los hiperpar치metros para el `Trainer`."],"metadata":{"id":"t3rzdbLGHCnk"}},{"cell_type":"code","source":["# Cargar m칠trica de evaluaci칩n (accuracy es com칰n para clasificaci칩n)\n","metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    # Obtener predicciones tomando el 칤ndice con mayor logit\n","    predictions = np.argmax(logits, axis=-1)\n","    # Calcular la m칠trica\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","# Definir argumentos para el entrenamiento\n","training_args = TrainingArguments(\n","    output_dir=\"./results_classification\", # Directorio donde guardar resultados/checkpoints\n","    evaluation_strategy=\"epoch\",         # Evaluar al final de cada 칠poca\n","    num_train_epochs=3,                  # N칰mero de 칠pocas (pocas para ejemplo r치pido)\n","    per_device_train_batch_size=2,       # Tama침o de batch peque침o para ejemplo\n","    per_device_eval_batch_size=2,\n","    warmup_steps=1,                      # Pasos de calentamiento (pocos para ejemplo)\n","    weight_decay=0.01,\n","    logging_dir=\"./logs_classification\",   # Directorio para logs\n","    logging_steps=1,\n","    # load_best_model_at_end=True, # Opcional: cargar el mejor modelo al final\n","    # push_to_hub=False, # Opcional: subir a Hugging Face Hub\n",")\n","\n","print(\"Argumentos de entrenamiento definidos.\")\n"],"metadata":{"id":"nL8KOrk1HGur"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. Crear y Ejecutar el Trainer**\n","\n","El `Trainer` de Hugging Face se encarga del bucle de entrenamiento y evaluaci칩n."],"metadata":{"id":"YNXj4XOVHRtb"}},{"cell_type":"code","source":["# Crear el objeto Trainer\n","trainer = Trainer(\n","    model=model,                         # El modelo a entrenar\n","    args=training_args,                  # Argumentos de entrenamiento\n","    train_dataset=tokenized_datasets[\"train\"], # Dataset de entrenamiento\n","    eval_dataset=tokenized_datasets[\"test\"],  # Dataset de evaluaci칩n\n","    compute_metrics=compute_metrics,     # Funci칩n para calcular m칠tricas\n","    tokenizer=tokenizer,                 # Tokenizador (칰til para padding din치mico si no se hizo antes)\n",")\n","\n","print(\"Trainer creado. Iniciando fine-tuning...\")\n","# Iniciar el entrenamiento (ajuste fino)\n","trainer.train()\n","\n","print(\"\\nFine-tuning completado.\")\n","\n","# Evaluar el modelo final\n","eval_results = trainer.evaluate()\n","print(\"\\nResultados de la evaluaci칩n final:\")\n","print(eval_results)\n"],"metadata":{"id":"m6pI9h-gHWsd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5. Guardar Modelo y Hacer Inferencia**\n","\n","Una vez entrenado, puedes guardar tu modelo ajustado y usarlo para predecir en nuevos datos."],"metadata":{"id":"jSlwtv9wHcbE"}},{"cell_type":"code","source":["# Guardar el modelo ajustado y el tokenizador\n","save_directory = \"./fine_tuned_distilbert_classifier\"\n","print(f\"\\nGuardando modelo en {save_directory}...\")\n","trainer.save_model(save_directory)\n","# El tokenizador tambi칠n se guarda autom치ticamente con save_model si se proporcion칩 al Trainer\n","# tokenizer.save_pretrained(save_directory) # Opcional si no se pas칩 al Trainer\n","print(\"Modelo guardado.\")\n","\n","# --- Cargar y usar para inferencia --- #\n","print(\"\\nCargando modelo guardado para inferencia...\")\n","# Cargar el modelo y tokenizador guardados\n","loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n","loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n","\n","# Mover modelo a GPU si est치 disponible (importante para inferencia r치pida)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","loaded_model.to(device)\n","loaded_model.eval() # Poner en modo evaluaci칩n\n","\n","# Texto nuevo para clasificar\n","new_text = \"This movie was incredibly moving and well-acted.\"\n","print(f\"\\nClasificando nuevo texto: '{new_text}'\")\n","\n","# Tokenizar el nuevo texto\n","inputs = loaded_tokenizer(new_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n","# Mover inputs a la misma GPU/CPU que el modelo\n","inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","# Hacer la predicci칩n\n","with torch.no_grad(): # No necesitamos gradientes para inferencia\n","    outputs = loaded_model(**inputs)\n","    logits = outputs.logits\n","\n","# Obtener la clase predicha\n","predicted_class_id = torch.argmax(logits, dim=-1).item()\n","\n","# Mapear ID a etiqueta (seg칰n nuestro dataset original)\n","predicted_label = \"Positivo\" if predicted_class_id == 1 else \"Negativo\"\n","\n","print(f\"Predicci칩n: {predicted_label} (ID: {predicted_class_id})\")\n","\n","# Ejemplo con otro texto\n","new_text_2 = \"A complete waste of time and money.\"\n","print(f\"\\nClasificando nuevo texto: '{new_text_2}'\")\n","inputs_2 = loaded_tokenizer(new_text_2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n","with torch.no_grad():\n","    outputs_2 = loaded_model(**inputs_2)\n","    logits_2 = outputs_2.logits\n","predicted_class_id_2 = torch.argmax(logits_2, dim=-1).item()\n","predicted_label_2 = \"Positivo\" if predicted_class_id_2 == 1 else \"Negativo\"\n","print(f\"Predicci칩n: {predicted_label_2} (ID: {predicted_class_id_2})\")\n"],"metadata":{"id":"9FK_-CecHnUS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Explicaci칩n Pr치ctica:**\n","\n","*   Hemos usado `transformers` y `datasets` para cargar un modelo pre-entrenado (DistilBERT), preparar un dataset simple y tokenizarlo.\n","*   Configuramos `TrainingArguments` y `Trainer` para manejar el bucle de fine-tuning y evaluaci칩n.\n","*   Entrenamos el modelo por unas pocas 칠pocas en nuestros datos espec칤ficos.\n","*   Guardamos el modelo ajustado.\n","*   Finalmente, cargamos el modelo guardado y lo usamos para clasificar nuevos textos.\n","*   **춰Experimenta!** Prueba con otros modelos pre-entrenados (`bert-base-uncased`, `roberta-base`), cambia los hiperpar치metros de entrenamiento, usa un dataset m치s grande de Hugging Face Hub (ej. `imdb`).\n","\n","---\n"],"metadata":{"id":"7tO9CujPHuJr"}},{"cell_type":"markdown","source":["## 5. Limitaciones y Consideraciones 칄ticas (Pr치cticas)\n","\n","Si bien los Transformers son potentes, es crucial ser consciente de sus limitaciones pr치cticas y las implicaciones 칠ticas al usarlos:\n","\n","**Limitaciones Pr치cticas:**\n","\n","*   **Recursos Computacionales:** Entrenar modelos grandes desde cero o incluso ajustar modelos muy grandes requiere GPUs/TPUs potentes y tiempo considerable. La inferencia tambi칠n puede ser costosa para modelos grandes.\n","*   **Longitud de Secuencia:** La complejidad cuadr치tica de la atenci칩n est치ndar limita la longitud de secuencia que se puede procesar eficientemente (aunque existen variantes como Longformer).\n","*   **\"Alucinaciones\" y Falta de Sentido Com칰n:** Los modelos pueden generar texto que suena plausible pero es incorrecto factualmente o carece de sentido com칰n. No \"entienden\" el mundo real, solo patrones estad칤sticos.\n","*   **Interpretabilidad:** Es dif칤cil saber exactamente *por qu칠* un Transformer genera una salida espec칤fica, lo que es problem치tico en aplicaciones cr칤ticas.\n","\n","**Consideraciones 칄ticas Pr치cticas:**\n","\n","*   **Sesgo (Bias):** Los modelos aprenden sesgos (sociales, de g칠nero, raciales) presentes en los datos de entrenamiento masivos. Esto puede llevar a resultados injustos o discriminatorios. *Acci칩n Pr치ctica:* Audita tus modelos en busca de sesgos, usa datasets m치s equilibrados si es posible, y considera t칠cnicas de mitigaci칩n de sesgos.\n","*   **Desinformaci칩n:** La capacidad de generar texto realista puede usarse para crear noticias falsas, spam, etc. *Acci칩n Pr치ctica:* S칠 responsable con el uso de modelos generativos y considera implementar salvaguardas.\n","*   **Consumo Energ칠tico:** Entrenar modelos grandes tiene una huella de carbono significativa. *Acci칩n Pr치ctica:* Prefiere usar modelos pre-entrenados y ajustarlos, considera modelos m치s peque침os/eficientes (como DistilBERT) o t칠cnicas de optimizaci칩n (ver Secci칩n 7) cuando sea posible.\n","*   **Privacidad:** Existe un riesgo (aunque bajo para modelos grandes) de que los modelos memoricen datos sensibles del entrenamiento. *Acci칩n Pr치ctica:* Aseg칰rate de que los datos de entrenamiento est칠n adecuadamente anonimizados, especialmente si son sensibles.\n","\n","---\n"],"metadata":{"id":"0WS9uyALHyiH"}},{"cell_type":"markdown","source":["## 6. Aplicaciones en Big Data (Conceptual Pr치ctico)\n","\n","쮺칩mo aplicar칤amos Transformers a conjuntos de datos masivos (Big Data), como millones de registros m칠dicos electr칩nicos (RME) para detectar eventos adversos a medicamentos?\n","\n","**Pasos Pr치cticos Conceptuales:**\n","\n","1.  **Infraestructura:** Necesitar치s un entorno de computaci칩n distribuida (ej. cl칰steres en la nube como AWS SageMaker, Google AI Platform, Azure ML) con acceso a m칰ltiples GPUs/TPUs.\n","2.  **Almacenamiento de Datos:** Los datos (RME de-identificados) probablemente residir치n en un data lake o almac칠n de datos distribuido (ej. S3, GCS, HDFS).\n","3.  **Preprocesamiento Distribuido:** Usar frameworks como **Apache Spark** (con PySpark) o **Dask** para:\n","    *   Leer datos en paralelo desde el almacenamiento distribuido.\n","    *   Realizar limpieza, de-identificaci칩n (춰crucial!), y tokenizaci칩n distribuida (posiblemente usando `datasets.map` con procesamiento distribuido si se integra con Spark/Dask, o UDFs de Spark con el tokenizador).\n","    *   Guardar los datos preprocesados/tokenizados de nuevo en formato eficiente (ej. Parquet).\n","4.  **Entrenamiento Distribuido:** Utilizar librer칤as que se integren con PyTorch para paralelizar el entrenamiento del Transformer (ej. ajuste fino de ClinicalBERT) en m칰ltiples GPUs/nodos:\n","    *   **PyTorch DistributedDataParallel (DDP):** Est치ndar de PyTorch para paralelismo de datos.\n","    *   **Hugging Face Accelerate:** Simplifica el uso de DDP y otras estrategias (como DeepSpeed) con los modelos y `Trainer` de Transformers.\n","    *   **Horovod:** Otro framework popular para entrenamiento distribuido.\n","    *   **DeepSpeed:** Librer칤a de Microsoft para entrenar modelos masivos, optimizando memoria y velocidad (paralelismo de datos, tensor, pipeline, optimizador ZeRO).\n","    *   **Ejemplo con Accelerate/Trainer:** El `Trainer` de Hugging Face se integra con `accelerate`. Simplemente lanzando tu script de entrenamiento con `accelerate launch tu_script.py --args...` en un entorno configurado, `accelerate` maneja la distribuci칩n.\n","5.  **Inferencia Distribuida:** Para aplicar el modelo entrenado a nuevos datos a gran escala, tambi칠n se pueden usar frameworks distribuidos (Spark UDFs con el modelo cargado, Dask) o servicios de inferencia optimizados (ej. NVIDIA Triton Inference Server, SageMaker Endpoints).\n","6.  **Monitorizaci칩n y Gesti칩n:** Usar herramientas para monitorizar el uso de recursos, el progreso del entrenamiento y gestionar los experimentos (ej. MLflow, Weights & Biases).\n","\n","**Consideraciones de C칩digo:**\n","\n","*   **Manejo de Memoria:** Cargar modelos grandes y batches de datos requiere GPUs con mucha VRAM. T칠cnicas como gradient accumulation (en `TrainingArguments`), precisi칩n mixta (`fp16=True`), y DeepSpeed (ZeRO) son esenciales.\n","*   **Eficiencia I/O:** Leer datos eficientemente desde el almacenamiento distribuido es clave. Formatos como Parquet o TFRecord son preferibles a archivos de texto plano.\n","*   **Robustez:** El c칩digo debe manejar fallos transitorios en el cl칰ster (ej. reintentos, checkpoints).\n"],"metadata":{"id":"h5rSAwCFH0Fr"}},{"cell_type":"code","source":["# Ejemplo conceptual (No ejecutable directamente aqu칤)\n","# Asume que tienes un script 'train_ehr.py' que usa Hugging Face Trainer\n","\n","# Configurar accelerate (una vez por nodo/m치quina)\n","# !accelerate config\n","\n","# Lanzar entrenamiento distribuido (ej. en 4 GPUs)\n","# !accelerate launch --num_processes=4 train_ehr.py \\\n","#   --output_dir ./ehr_output \\\n","#   --model_name_or_path emilyalsentzer/Bio_ClinicalBERT \\\n","#   --train_file path/to/distributed/train.parquet \\\n","#   --validation_file path/to/distributed/eval.parquet \\\n","#   --do_train --do_eval \\\n","#   --num_train_epochs 1 \\\n","#   --per_device_train_batch_size 8 \\\n","#   --fp16 # Usar precisi칩n mixta\n","\n","print(\"Conceptual: Lanzamiento de entrenamiento distribuido con Accelerate.\")\n"],"metadata":{"id":"bGVe3jCeH6Kc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Desaf칤os Clave:** Privacidad (HIPAA/GDPR), heterogeneidad de datos m칠dicos, coste computacional, interpretabilidad.\n","\n","---\n"],"metadata":{"id":"-LKD2krDH_lK"}},{"cell_type":"markdown","source":["## 7. Optimizaci칩n y Eficiencia (Pr치ctica Conceptual)\n","\n","Los modelos Transformer grandes pueden ser lentos y consumir mucha memoria. Aqu칤 hay un vistazo pr치ctico a c칩mo optimizarlos:\n","\n","**T칠cnicas Comunes:**\n","\n","*   **Cuantizaci칩n (Quantization):** Reduce la precisi칩n de los n칰meros (pesos/activaciones) de 32 bits (FP32) a 16 bits (FP16/BF16) o 8 bits (INT8). Esto reduce el tama침o del modelo (~4x para INT8) y acelera la inferencia en hardware compatible (~2-4x).\n","*   **Poda (Pruning):** Elimina pesos o estructuras menos importantes del modelo. Puede reducir significativamente el tama침o y los c치lculos, a veces con poca p칠rdida de precisi칩n.\n","*   **Destilaci칩n del Conocimiento (Knowledge Distillation):** Entrena un modelo m치s peque침o (\"estudiante\") para imitar a uno m치s grande (\"profesor\"). El estudiante aprende de las predicciones del profesor, logrando un buen rendimiento con menos tama침o/velocidad (ej. DistilBERT).\n","*   **Arquitecturas Eficientes:** Usar variantes de Transformer dise침adas para ser m치s r치pidas o manejar secuencias largas (ej. Longformer, Linformer, Reformer).\n","\n","### 7.1. Ejemplo Pr치ctico: Cuantizaci칩n con PyTorch\n","\n","PyTorch ofrece herramientas para cuantizaci칩n post-entrenamiento (PTQ) y entrenamiento consciente de la cuantizaci칩n (QAT)."],"metadata":{"id":"RXp3a8OIIE9c"}},{"cell_type":"code","source":["import torch\n","import torch.quantization\n","import copy\n","\n","# --- Cuantizaci칩n Din치mica Post-Entrenamiento (PTQ) ---\n","# M치s simple, buena para LSTMs/Transformers, cuantiza solo pesos\n","\n","# Carga tu modelo FP32 entrenado (usaremos una red lineal simple como ejemplo)\n","# En la pr치ctica, cargar칤as tu modelo Transformer ajustado\n","fp32_model = nn.Sequential(\n","    nn.Linear(128, 256),\n","    nn.ReLU(),\n","    nn.Linear(256, 10)\n",")\n","fp32_model.eval() # Poner en modo evaluaci칩n\n","\n","# Aplicar cuantizaci칩n din치mica (para capas Lineales y RNNs)\n","# backend=\\'qnnpack\\' es com칰n para ARM, \\'fbgemm\\' para x86\n","quantized_dynamic_model = torch.quantization.quantize_dynamic(\n","    fp32_model, {nn.Linear}, dtype=torch.qint8, inplace=False\n",")\n","\n","print(\"Modelo original (FP32):\")\n","# print(fp32_model)\n","print(\"\\nModelo cuantizado din치micamente (INT8 pesos para Lineales):\")\n","# print(quantized_dynamic_model)\n","\n","# Comparar tama침o (conceptual)\n","# import os\n","# torch.save(fp32_model.state_dict(), \"fp32_model.pth\")\n","# torch.save(quantized_dynamic_model.state_dict(), \"quantized_dynamic_model.pth\")\n","# print(f\"Tama침o FP32: {os.path.getsize(\\'fp32_model.pth\\') / 1e6:.2f} MB\")\n","# print(f\"Tama침o INT8 Din치mico: {os.path.getsize(\\'quantized_dynamic_model.pth\\') / 1e6:.2f} MB\") # Deber칤a ser ~1/4 para pesos lineales\n","\n","# --- Cuantizaci칩n Est치tica Post-Entrenamiento (PTQ) ---\n","# Requiere calibraci칩n con datos, cuantiza pesos y activaciones\n","\n","# Clona el modelo original\n","static_model = copy.deepcopy(fp32_model)\n","static_model.eval()\n","\n","# Especificar configuraci칩n de cuantizaci칩n\n","static_model.qconfig = torch.quantization.get_default_qconfig(\\'fbgemm\\') # o \\'qnnpack\\'\n","\n","# Fusionar m칩dulos (Conv-BN-ReLU, etc.) si aplica (no en este ejemplo simple)\n","# static_model_fused = torch.quantization.fuse_modules(static_model, [['0', '1']], inplace=False)\n","\n","# Preparar el modelo para la calibraci칩n\n","static_model_prepared = torch.quantization.prepare(static_model, inplace=False)\n","\n","# Calibrar con datos representativos (ej. algunos batches del dataset de validaci칩n)\n","# print(\"\\nCalibrando modelo est치tico...\")\n","# with torch.no_grad():\n","#     for i in range(10): # Usar datos reales aqu칤\n","#         dummy_input = torch.randn(1, 128)\n","#         static_model_prepared(dummy_input)\n","\n","# Convertir a modelo cuantizado\n","# static_model_quantized = torch.quantization.convert(static_model_prepared, inplace=False)\n","# print(\"Modelo cuantizado est치ticamente (INT8 pesos y activaciones):\")\n","# print(static_model_quantized)\n","\n","# --- Entrenamiento Consciente de Cuantizaci칩n (QAT) ---\n","# Mejor precisi칩n, simula cuantizaci칩n durante el entrenamiento\n","\n","# qat_model = copy.deepcopy(fp32_model)\n","# qat_model.train() # Poner en modo entrenamiento\n","# qat_model.qconfig = torch.quantization.get_default_qat_qconfig(\\'fbgemm\\')\n","# qat_model_fused = torch.quantization.fuse_modules(qat_model, [['0', '1']], inplace=False)\n","# qat_model_prepared = torch.quantization.prepare_qat(qat_model_fused, inplace=False)\n","\n","# Entrenar el modelo preparado (qat_model_prepared) por algunas 칠pocas...\n","# print(\"\\n(Conceptual) Entrenando con QAT...\")\n","\n","# Despu칠s del entrenamiento, convertir a modelo cuantizado\n","# qat_model_prepared.eval()\n","# qat_model_quantized = torch.quantization.convert(qat_model_prepared, inplace=False)\n","# print(\"(Conceptual) Modelo final cuantizado con QAT:\")\n","# print(qat_model_quantized)\n","\n","print(\"\\nEjemplos conceptuales de cuantizaci칩n con PyTorch.\")\n"],"metadata":{"id":"qIeVjsiZIM-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 7.2. Hugging Face Optimum\n","\n","La librer칤a `optimum` de Hugging Face simplifica la aplicaci칩n de t칠cnicas de optimizaci칩n (cuantizaci칩n, ONNX Runtime) a los modelos de `transformers`."],"metadata":{"id":"h1FYQ0PnIR4r"}},{"cell_type":"code","source":["# !pip install optimum[onnxruntime]\n","\n","from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification\n","from optimum.onnxruntime.configuration import AutoQuantizationConfig\n","\n","# Directorio del modelo ajustado de la secci칩n anterior\n","model_checkpoint_dir = \"./fine_tuned_distilbert_classifier\"\n","# Directorio de salida para el modelo ONNX cuantizado\n","onnx_quantized_output_dir = \"./onnx_quantized_distilbert\"\n","\n","# --- Cuantizaci칩n Post-Entrenamiento con Optimum --- #\n","\n","# 1. Crear cuantizador desde el modelo ajustado\n","ort_quantizer = ORTQuantizer.from_pretrained(model_checkpoint_dir)\n","\n","# 2. Definir configuraci칩n de cuantizaci칩n (ej. AVX2 para CPU, INT8)\n","# qconfig = AutoQuantizationConfig.avx2(config=None, is_static=False) # Din치mica\n","qconfig = AutoQuantizationConfig.avx2(config=None, is_static=True) # Est치tica (requiere dataset de calibraci칩n)\n","\n","# 3. (Solo para est치tica) Cargar dataset de calibraci칩n\n","# from datasets import load_dataset\n","# calibration_dataset = ort_quantizer.get_calibration_dataset(\n","#     \"glue\", # Ejemplo: dataset GLUE\n","#     dataset_config_name=\"sst2\", # Ejemplo: tarea SST-2\n","#     preprocess_function=lambda examples: tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True),\n","#     num_samples=50, # N칰mero de muestras para calibrar\n","#     dataset_split=\"train\",\n","# )\n","\n","# 4. Cuantizar a formato ONNX\n","# Para est치tica, a침adir: calibration_dataset=calibration_dataset\n","ort_quantizer.quantize(\n","    save_dir=onnx_quantized_output_dir,\n","    quantization_config=qconfig,\n",")\n","\n","print(f\"Modelo cuantizado y exportado a ONNX en {onnx_quantized_output_dir}\")\n","\n","# --- Cargar y usar modelo ONNX cuantizado --- #\n","# print(\"\\nCargando modelo ONNX cuantizado...\")\n","# loaded_onnx_model = ORTModelForSequenceClassification.from_pretrained(onnx_quantized_output_dir)\n","# loaded_onnx_tokenizer = AutoTokenizer.from_pretrained(onnx_quantized_output_dir)\n","\n","# new_text = \"This is a great tool!\"\n","# inputs = loaded_onnx_tokenizer(new_text, return_tensors=\"pt\")\n","\n","# outputs = loaded_onnx_model(**inputs)\n","# logits = outputs.logits\n","# predicted_class_id = torch.argmax(logits, dim=-1).item()\n","# print(f\"Predicci칩n ONNX: {\"Positivo\" if predicted_class_id == 1 else \"Negativo\"}\")\n","\n"],"metadata":{"id":"bf38KBM2IW-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Explicaci칩n Pr치ctica:**\n","\n","*   PyTorch ofrece APIs para cuantizaci칩n din치mica, est치tica y QAT. La din치mica es la m치s simple, la est치tica requiere calibraci칩n, y QAT da la mejor precisi칩n pero requiere reentrenamiento.\n","*   Hugging Face `optimum` simplifica la cuantizaci칩n (especialmente a formato ONNX para inferencia acelerada con ONNX Runtime) de modelos `transformers`.\n","*   La elecci칩n depende del hardware objetivo (CPU/GPU/TPU), los requisitos de precisi칩n y la complejidad de implementaci칩n aceptable.\n","\n","---"],"metadata":{"id":"tyqMR42wIchK"}},{"cell_type":"markdown","source":["## 8. Dise침o Personalizado: Resumen Abstractivo con 游뱅 Transformers\n","\n","Dise침ar un modelo \"personalizado\" a menudo significa adaptar una arquitectura pre-entrenada para una tarea espec칤fica. Vamos a usar un modelo pre-entrenado Encoder-Decoder (como BART o PEGASUS) para **resumen abstractivo** (generar un resumen que no solo copia frases).\n","\n","**Tarea:** Dado un texto largo (ej. art칤culo), generar un resumen corto.\n","\n","**Modelo:** Usaremos `google/pegasus-xsum`, un modelo pre-entrenado espec칤ficamente para res칰menes cortos y abstractivos (entrenado en el dataset XSum).\n"],"metadata":{"id":"EFRaQth7Ipra"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import torch\n","\n","# Cargar modelo y tokenizador pre-entrenado para resumen\n","model_name_summarization = \"google/pegasus-xsum\"\n","print(f\"Cargando modelo {model_name_summarization}...\")\n","\n","# Usar try-except por si hay problemas de conexi칩n o memoria en Colab\n","try:\n","    tokenizer_summarization = AutoTokenizer.from_pretrained(model_name_summarization)\n","    model_summarization = AutoModelForSeq2SeqLM.from_pretrained(model_name_summarization)\n","\n","    # Mover a GPU si est치 disponible\n","    device_sum = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model_summarization.to(device_sum)\n","    model_summarization.eval() # Modo evaluaci칩n\n","\n","    print(\"Modelo de resumen cargado.\")\n","\n","    # Texto de ejemplo para resumir (puedes reemplazarlo)\n","    ARTICLE_TO_SUMMARIZE = (\"\"\"\n","    Scientists have discovered a new species of deep-sea fish living near hydrothermal vents\n","    in the Pacific Ocean. The fish, temporarily named Ventichthys 혢혞햢 (chudo, meaning wonder),\n","    possesses unique bioluminescent features never before seen. Adapted to extreme pressure\n","    and temperature, its discovery challenges existing theories about the limits of life\n","    on Earth. Researchers used a remotely operated vehicle (ROV) to capture footage and\n","    samples at depths exceeding 3,000 meters. Further genetic analysis is underway to\n","    understand its evolutionary lineage and unique adaptations. The finding highlights\n","    how much of the deep ocean remains unexplored and the potential for discovering\n","    entirely new ecosystems and life forms.\n","    \"\"\")\n","\n","    print(f\"\\nTexto a resumir:\\n{ARTICLE_TO_SUMMARIZE}\")\n","\n","    # Tokenizar el texto de entrada\n","    inputs_summarization = tokenizer_summarization(ARTICLE_TO_SUMMARIZE,\n","                                                   max_length=1024, # Longitud m치xima de entrada para PEGASUS\n","                                                   return_tensors=\"pt\",\n","                                                   truncation=True).to(device_sum)\n","\n","    # Generar el resumen\n","    # Par치metros de generaci칩n comunes:\n","    # - max_length: Longitud m치xima del resumen generado\n","    # - min_length: Longitud m칤nima del resumen generado\n","    # - num_beams: Usa beam search para mejor calidad (m치s lento)\n","    # - length_penalty: Penaliza res칰menes m치s largos/cortos\n","    # - no_repeat_ngram_size: Evita repetir n-gramas\n","    print(\"\\nGenerando resumen...\")\n","    with torch.no_grad():\n","        summary_ids = model_summarization.generate(inputs_summarization[\"input_ids\"],\n","                                                 num_beams=4,\n","                                                 min_length=30,\n","                                                 max_length=60, # Resumen corto para XSum\n","                                                 early_stopping=True)\n","\n","    # Decodificar los IDs generados a texto\n","    generated_summary = tokenizer_summarization.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    print(f\"\\nResumen Generado:\\n{generated_summary}\")\n","\n","except Exception as e:\n","    print(f\"Error al cargar o usar el modelo de resumen: {e}\")\n","    print(\"Esto puede deberse a memoria insuficiente en Colab o problemas de red.\")\n","    print(\"Intenta reiniciar el entorno de ejecuci칩n o usar un modelo m치s peque침o si es necesario.\")\n","\n"],"metadata":{"id":"z_k-HNHmIq9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","**Explicaci칩n Pr치ctica:**\n","\n","*   Cargamos un modelo Seq2Seq (`AutoModelForSeq2SeqLM`) y su tokenizador, espec칤ficamente `pegasus-xsum` que est치 optimizado para res칰menes cortos.\n","*   Tokenizamos el art칤culo de entrada.\n","*   Usamos el m칠todo `model.generate()` para producir el resumen. Este m칠todo implementa la decodificaci칩n (a menudo usando beam search) para generar la secuencia de salida token por token.\n","*   Ajustamos los par치metros de `generate` (como `num_beams`, `max_length`, `min_length`) para controlar la calidad y longitud del resumen.\n","*   Finalmente, decodificamos los IDs de token generados para obtener el resumen en texto.\n","*   **춰Experimenta!** Prueba con diferentes textos, ajusta los par치metros de `generate`, o incluso intenta con otro modelo pre-entrenado para resumen (ej. `google/bart-large-cnn`).\n","\n","---\n","\n","## Conclusi칩n y Pr칩ximos Pasos\n","\n","Este cuaderno ha proporcionado una introducci칩n pr치ctica a los Transformers con PyTorch, cubriendo desde la implementaci칩n b치sica hasta el uso avanzado con modelos pre-entrenados y t칠cnicas de optimizaci칩n.\n","\n","**Puntos Clave:**\n","\n","*   Los Transformers se basan en la **atenci칩n** para capturar relaciones en los datos.\n","*   Son altamente **paralelizables**, permitiendo modelos muy grandes.\n","*   El **pre-entrenamiento y ajuste fino** con librer칤as como 游뱅 Transformers es el enfoque est치ndar.\n","*   Existen **limitaciones** (recursos, longitud de secuencia) y **consideraciones 칠ticas** (sesgo, desinformaci칩n) importantes.\n","*   Las t칠cnicas de **optimizaci칩n** (cuantizaci칩n, poda) y las **arquitecturas eficientes** ayudan a desplegar modelos en entornos pr치cticos.\n","\n","**Pr칩ximos Pasos:**\n","\n","*   Profundiza en tareas espec칤ficas (Q&A, NER, Traducci칩n) usando `transformers`.\n","*   Explora diferentes modelos pre-entrenados en [Hugging Face Hub](https://huggingface.co/models).\n","*   Aprende m치s sobre t칠cnicas de optimizaci칩n con `optimum` o PyTorch.\n","*   Intenta entrenar un modelo desde cero (si tienes suficientes datos y recursos) o ajustar modelos m치s grandes.\n","*   Contribuye a la comunidad de c칩digo abierto.\n","\n","춰Esperamos que esta gu칤a pr치ctica te haya sido 칰til en tu viaje de aprendizaje sobre Transformers!\n","\n","---\n","\n","## Referencias (Simplificado)\n","\n","*   Vaswani et al. (2017). Attention is all you need. [Link](https://arxiv.org/abs/1706.03762)\n","*   Devlin et al. (2018). BERT: Pre-training of Deep Bidirectional Transformers... [Link](https://arxiv.org/abs/1810.04805)\n","*   Radford et al. (GPT series). OpenAI Blog.\n","*   Raffel et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. [Link](https://arxiv.org/abs/1910.10683)\n","*   Wolf et al. (2019). HuggingFace's Transformers: State-of-the-Art Natural Language Processing. [Link](https://arxiv.org/abs/1910.03771)\n","*   Zhang et al. (2020). PEGASUS: Pre-training with Extracted Gap-sentences... [Link](https://arxiv.org/abs/1912.08777)\n","*   PyTorch Documentation: [Link](https://pytorch.org/docs/stable/index.html)\n","*   Hugging Face Documentation (Transformers, Datasets, Evaluate, Optimum): [Link](https://huggingface.co/docs)\n","\n","---\n","\n"],"metadata":{"id":"WD4cDXtNIvHC"}},{"cell_type":"markdown","source":["## Otra pr치ctica"],"metadata":{"id":"TuFA1pPohBmF"}},{"cell_type":"markdown","source":["La estructura base del Transformer sencillo en PyTorch"],"metadata":{"id":"UlnQEZCwhSdA"}},{"cell_type":"code","source":["# Importamos librerias\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Clase MultiHeadAttention: Implementa la atenci칩n multi-cabeza\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, n_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        # Verificamos que d_model sea divisible por el n칰mero de cabezas\n","        assert d_model % n_heads == 0\n","        self.d_k = d_model // n_heads  # Dimensi칩n de cada cabeza\n","        self.n_heads = n_heads\n","\n","        # Definimos capas lineales para Q, K, V y la salida\n","        self.q_linear = nn.Linear(d_model, d_model)\n","        self.k_linear = nn.Linear(d_model, d_model)\n","        self.v_linear = nn.Linear(d_model, d_model)\n","        self.out_linear = nn.Linear(d_model, d_model)\n","\n","    def attention(self, Q, K, V):\n","        # Calculamos los scores de atenci칩n escalados\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n","        # Aplicamos softmax para obtener los pesos de atenci칩n\n","        attn_weights = F.softmax(scores, dim=-1)\n","        # Calculamos la salida ponderada\n","        output = torch.matmul(attn_weights, V)\n","        return output, attn_weights\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","\n","        # Aplicamos las capas lineales para obtener Q, K, V\n","        Q = self.q_linear(x)\n","        K = self.k_linear(x)\n","        V = self.v_linear(x)\n","\n","        # Redimensionamos para aplicar atenci칩n multi-cabeza\n","        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n","        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n","        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n","\n","        # Llamamos a la funci칩n de atenci칩n\n","        attn_output, _ = self.attention(Q, K, V)\n","\n","        # Volvemos a unir las cabezas de atenci칩n\n","        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_k * self.n_heads)\n","        # Aplicamos la capa de salida\n","        output = self.out_linear(attn_output)\n","        return output\n","\n","# Clase FeedForward: Implementa el MLP del Transformer\n","class FeedForward(nn.Module):\n","    def __init__(self, d_model, ff_dim, dropout=0.1):\n","        super(FeedForward, self).__init__()\n","        # Capa intermedia con ReLU\n","        self.fc1 = nn.Linear(d_model, ff_dim)\n","        # Capa de salida\n","        self.fc2 = nn.Linear(ff_dim, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # Aplicamos ReLU y Dropout\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        # Capa de salida\n","        x = self.fc2(x)\n","        return x\n","\n","# Bloque Transformer: Implementa una capa de Transformer\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n","        super(TransformerBlock, self).__init__()\n","        # M칩dulo de atenci칩n multi-cabeza\n","        self.attention = MultiHeadAttention(d_model, n_heads)\n","        # Normalizaci칩n de la salida de la atenci칩n\n","        self.norm1 = nn.LayerNorm(d_model)\n","        # M칩dulo feedforward\n","        self.ff = FeedForward(d_model, ff_dim, dropout)\n","        # Normalizaci칩n de la salida del feedforward\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # Aplicamos atenci칩n y normalizaci칩n\n","        attn_output = self.attention(x)\n","        x = self.norm1(x + self.dropout(attn_output))\n","        # Aplicamos feedforward y normalizaci칩n\n","        ff_output = self.ff(x)\n","        x = self.norm2(x + self.dropout(ff_output))\n","        return x\n","\n","# Modelo Transformer Simple\n","class SimpleTransformer(nn.Module):\n","    def __init__(self, input_dim, d_model, n_heads, ff_dim, n_layers, dropout=0.1):\n","        super(SimpleTransformer, self).__init__()\n","        # Capa de embedding inicial\n","        self.embedding = nn.Linear(input_dim, d_model)\n","        # M칰ltiples capas Transformer\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n","        ])\n","        # Capa de salida\n","        self.fc_out = nn.Linear(d_model, input_dim)\n","\n","    def forward(self, x):\n","        # Aplicamos el embedding inicial\n","        x = self.embedding(x)\n","        # Pasamos por cada capa Transformer\n","        for layer in self.layers:\n","            x = layer(x)\n","        # Capa de salida final\n","        output = self.fc_out(x)\n","        return output\n","\n","# Definici칩n de hiperpar치metros\n","input_dim = 10  # Dimensi칩n de entrada\n","\n","# Par치metros del modelo\n","d_model = 128\n","n_heads = 4\n","ff_dim = 512\n","n_layers = 2\n","\n","# Inicializaci칩n del modelo\n","model = SimpleTransformer(input_dim, d_model, n_heads, ff_dim, n_layers)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blhjDnAdhG1r","executionInfo":{"status":"ok","timestamp":1746634540122,"user_tz":-60,"elapsed":29,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"a10c79c3-784f-4f79-f4c6-a8d93dd0396b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleTransformer(\n","  (embedding): Linear(in_features=10, out_features=128, bias=True)\n","  (layers): ModuleList(\n","    (0-1): 2 x TransformerBlock(\n","      (attention): MultiHeadAttention(\n","        (q_linear): Linear(in_features=128, out_features=128, bias=True)\n","        (k_linear): Linear(in_features=128, out_features=128, bias=True)\n","        (v_linear): Linear(in_features=128, out_features=128, bias=True)\n","        (out_linear): Linear(in_features=128, out_features=128, bias=True)\n","      )\n","      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (ff): FeedForward(\n","        (fc1): Linear(in_features=128, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=128, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","  )\n","  (fc_out): Linear(in_features=128, out_features=10, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["Traductor de idiomas"],"metadata":{"id":"gZodcrhsjUqY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Clase AttentionMechanism: Implementa un mecanismo de atenci칩n simple\n","class AttentionMechanism(nn.Module):\n","    def __init__(self, d_model):\n","        super(AttentionMechanism, self).__init__()\n","        # Capas lineales para Query, Key y Value\n","        self.query_layer = nn.Linear(d_model, d_model)\n","        self.key_layer = nn.Linear(d_model, d_model)\n","        self.value_layer = nn.Linear(d_model, d_model)\n","\n","    def forward(self, query, key, value):\n","        # Aplicamos capas lineales para obtener Q, K, V\n","        Q = self.query_layer(query)  # Transformamos el Query\n","        K = self.key_layer(key)      # Transformamos el Key\n","        V = self.value_layer(value)  # Transformamos el Value\n","\n","        # Calculamos los scores de atenci칩n\n","        # Multiplicamos Q por la transpuesta de K y escalamos por la ra칤z cuadrada de la dimensi칩n\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))\n","\n","        # Aplicamos softmax para convertir los scores en probabilidades de atenci칩n\n","        attn_weights = F.softmax(scores, dim=-1)\n","\n","        # Multiplicamos los pesos de atenci칩n por los valores V\n","        output = torch.matmul(attn_weights, V)\n","\n","        return output, attn_weights\n","\n","# Ejemplo pr치ctico: Traducci칩n de ingl칠s a espa침ol\n","# Supongamos que tenemos un diccionario simple con palabras embebidas\n","embedding_dim = 16  # Dimensi칩n del embedding\n","\n","# Secuencia en ingl칠s (5 palabras representadas como vectores de 16 dimensiones)\n","english_sentence = torch.randn((1, 5, embedding_dim))  # (batch_size, seq_len, embedding_dim)\n","\n","# Secuencia en espa침ol (7 palabras representadas como vectores de 16 dimensiones)\n","spanish_sentence = torch.randn((1, 7, embedding_dim))  # (batch_size, seq_len, embedding_dim)\n","\n","# Inicializamos el mecanismo de atenci칩n\n","attention = AttentionMechanism(embedding_dim)\n","\n","# Aplicamos atenci칩n utilizando la oraci칩n en ingl칠s como Query y la oraci칩n en espa침ol como Key/Value\n","output, attn_weights = attention(english_sentence, spanish_sentence, spanish_sentence)\n","\n","# Salida del mecanismo de atenci칩n\n","print(\"Output Shape (context vector):\", output.shape)  # (batch_size, seq_len_english, embedding_dim)\n","print(\"Attention Weights Shape:\", attn_weights.shape)  # (batch_size, seq_len_english, seq_len_spanish)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drCpfwkajY2h","executionInfo":{"status":"ok","timestamp":1746635016944,"user_tz":-60,"elapsed":14,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"4bd862d9-eb51-4ac8-fa6b-a5484a8b1c1a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Output Shape (context vector): torch.Size([1, 5, 16])\n","Attention Weights Shape: torch.Size([1, 5, 7])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from transformers import BertTokenizer\n","\n","# Clase TransformerEncoder: Implementa el Encoder del modelo Transformer\n","class TransformerEncoder(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n","        super(TransformerEncoder, self).__init__()\n","        # M칩dulo de Auto-Atenci칩n Multi-Cabeza\n","        self.self_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n","        # Capa Feed Forward\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, ff_dim),  # Proyecci칩n a mayor dimensi칩n\n","            nn.ReLU(),  # Funci칩n de activaci칩n no lineal\n","            nn.Dropout(dropout),  # Regularizaci칩n\n","            nn.Linear(ff_dim, d_model)  # Proyecci칩n de vuelta a la dimensi칩n original\n","        )\n","        # Capas de Normalizaci칩n\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        # Paso 1: Auto-atenci칩n sobre la entrada\n","        attn_output, _ = self.self_attention(x, x, x)\n","        # Residual + Normalizaci칩n\n","        x = self.norm1(x + attn_output)\n","        # Paso 2: Feedforward\n","        ff_output = self.ff(x)\n","        # Residual + Normalizaci칩n\n","        x = self.norm2(x + ff_output)\n","        return x\n","\n","# Clase TransformerDecoder: Implementa el Decoder del Transformer\n","class TransformerDecoder(nn.Module):\n","    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n","        super(TransformerDecoder, self).__init__()\n","        # Auto-Atenci칩n del Decoder\n","        self.self_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n","        # Atenci칩n Cruzada con la salida del Encoder\n","        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, dropout=dropout)\n","        # Capa Feed Forward\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, ff_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(ff_dim, d_model)\n","        )\n","        # Capas de Normalizaci칩n\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","        self.norm3 = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, enc_output):\n","        # Auto-atenci칩n del Decoder\n","        attn_output, _ = self.self_attention(x, x, x)\n","        x = self.norm1(x + attn_output)\n","        # Atenci칩n Cruzada con la salida del Encoder\n","        cross_output, _ = self.cross_attention(x, enc_output, enc_output)\n","        x = self.norm2(x + cross_output)\n","        # Feedforward\n","        ff_output = self.ff(x)\n","        x = self.norm3(x + ff_output)\n","        return x\n","\n","# Clase TransformerSummarizer: Implementa el modelo completo para resumen\n","class TransformerSummarizer(nn.Module):\n","    def __init__(self, input_dim, d_model, n_heads, ff_dim, n_layers, dropout=0.1):\n","        super(TransformerSummarizer, self).__init__()\n","        # Capa de Embedding para convertir tokens en vectores\n","        self.embedding = nn.Embedding(input_dim, d_model)\n","        # Lista de capas Encoder\n","        self.encoder_layers = nn.ModuleList([\n","            TransformerEncoder(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n","        ])\n","        # Lista de capas Decoder\n","        self.decoder_layers = nn.ModuleList([\n","            TransformerDecoder(d_model, n_heads, ff_dim, dropout) for _ in range(n_layers)\n","        ])\n","        # Capa de salida para generar las palabras del resumen\n","        self.output_layer = nn.Linear(d_model, input_dim)\n","\n","    def forward(self, src, tgt):\n","        # Embedding de entrada (texto original)\n","        src = self.embedding(src)\n","        # Embedding de la secuencia objetivo (resumen)\n","        tgt = self.embedding(tgt)\n","        # Paso por las capas del Encoder\n","        for layer in self.encoder_layers:\n","            src = layer(src)\n","        # Paso por las capas del Decoder\n","        for layer in self.decoder_layers:\n","            tgt = layer(tgt, src)\n","        # Generaci칩n de la secuencia de salida\n","        output = self.output_layer(tgt)\n","        return output\n","\n","# Definici칩n de hiperpar치metros\n","input_dim = 30522  # Tama침o del vocabulario BERT (ejemplo)\n","d_model = 256\n","n_heads = 8\n","ff_dim = 512\n","n_layers = 4\n","\n","# Inicializaci칩n del modelo con los hiperpar치metros definidos\n","model = TransformerSummarizer(input_dim, d_model, n_heads, ff_dim, n_layers)\n","print(model)  # Visualizaci칩n de la estructura del modelo\n"],"metadata":{"id":"LEsyxOhjmqTS","executionInfo":{"status":"ok","timestamp":1746635770756,"user_tz":-60,"elapsed":3109,"user":{"displayName":"FRANCISCO JOSE GONZALEZ RODRIGUEZ","userId":"13110676490913621747"}},"outputId":"2c715581-d85b-4b61-c0f4-baa72c356f0e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["TransformerSummarizer(\n","  (embedding): Embedding(30522, 256)\n","  (encoder_layers): ModuleList(\n","    (0-3): 4 x TransformerEncoder(\n","      (self_attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (ff): Sequential(\n","        (0): Linear(in_features=256, out_features=512, bias=True)\n","        (1): ReLU()\n","        (2): Dropout(p=0.1, inplace=False)\n","        (3): Linear(in_features=512, out_features=256, bias=True)\n","      )\n","      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (decoder_layers): ModuleList(\n","    (0-3): 4 x TransformerDecoder(\n","      (self_attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (cross_attention): MultiheadAttention(\n","        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n","      )\n","      (ff): Sequential(\n","        (0): Linear(in_features=256, out_features=512, bias=True)\n","        (1): ReLU()\n","        (2): Dropout(p=0.1, inplace=False)\n","        (3): Linear(in_features=512, out_features=256, bias=True)\n","      )\n","      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (output_layer): Linear(in_features=256, out_features=30522, bias=True)\n",")\n"]}]}]}