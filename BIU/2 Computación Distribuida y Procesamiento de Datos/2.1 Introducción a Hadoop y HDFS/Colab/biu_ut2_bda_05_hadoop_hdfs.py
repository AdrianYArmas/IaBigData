# -*- coding: utf-8 -*-
"""BIU_UT2_BDA_05_Hadoop_HDFS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oGnSiLD5Nx5MXhnOfB4jubNGsj8nUdId

# Google Colab

Google Colab es una herramienta online gratuita basada en la nube que permite desplegar modelos de aprendizaje automático de forma remota en **CPUs y GPUs**

Se basa en la tecnología de código abierto **Jupyter**, con la que se puede crear un cuaderno **Ipython**. El cuaderno Ipython no sólo te ofrece la posibilidad de escribir código, sino también de *contar una historia* a través de él.

Puedes ejecutar código, crear visualizaciones dedatos, y escribir sobre cada paso que se haga en texto plano o markdown. Esto hace que el código incluya explicaciones y visualizaciones de lo que hace.

¿Que recursos ofrece Colab?

- Uso de Disco
"""

!df -h

"""- CPU's"""

!cat /proc/cpuinfo | grep "model name"

"""* Memoria"""

!cat /proc/meminfo | grep "Mem"*

"""* GPU

Si activamos el soporte GPU entonces podremos obtener información sobre la misma
Información sobre la GPU disponible

`nvidia-smi -L` solo lista el nombre

`nvidia-smi -q ` mucha más información disponible
"""

!nvidia-smi

!nvcc --version

!cat /proc/driver/nvidia/gpus/0000:00:04.0/information

"""# Hadoop

Hadoop es un marco de programación basado en Java que permite procesar y almacenar conjuntos de datos extremadamente grandes en un clúster de máquinas de bajo coste. Fue el primer gran proyecto de código abierto en el ámbito del Big Data y está patrocinado por la Apache Software Foundation.

## Instalación de Hadoop

* bajar la distribución correspondiente
"""

!wget https://downloads.apache.org/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz

"""* extraerla en el sistema de archivos de colab"""

!tar -xzf hadoop-3.4.1.tar.gz

"""* mover la distribución de hadoop  /usr/local"""

!mv  hadoop-3.4.1/ /usr/local/

"""## Configuración

* actualizamos variables de entorno (JAVA_HOME, PATH)
"""

import os
os.environ["JAVA_HOME"]="/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] = os.environ["PATH"] + ":" + "/usr/local/hadoop-3.4.1/bin"

"""* comprobamos instalación"""

!hadoop version

"""## Ejecución de ejemplos

Una de las formas tradicionales de asegurarnos que un ambiente de Hadoop recién instalado funciona correctamente, es ejecutando el *jar* de ejemplos *map-reduce* incluido con toda instalación de hadoop (*hadoop-mapreduce-examples.jar*).

[Hadoop Map Reduce Examples](http://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/)

1. Creamos un directorio de ficheros en los que volquemos los xml de hadoop
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# mkdir ~/input
# cp /usr/local/hadoop-3.4.1/etc/hadoop/*.xml ~/input
# ls ~/input

"""2. Ejecutamos hadoop jar con el fin de ejecutar uno de los ejemplos por defecto, en este caso el grep que busca expresiones regulares dentro de los ficheros que le especifiquemos."""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# hadoop jar \
#   /usr/local/hadoop-3.4.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.1.jar \
#   grep ~/input ~/grep_example 'allowed[.]*'

!cat ~/grep_example/*

"""# HDFS

Las siguientes sentencias únicamente sirven para probar comandos básicos de HDFS no para gestionar una Infraestructura que en Google Colab no existe, en este caso el sistema de archivos HDFS es el mismo que el local

* Crear el directorio *prueba*
"""

!hdfs dfs -mkdir prueba

"""- Crear un fichero local :"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# echo "Ejemplo de HDFS" > user.txt
# echo `date` >> user.txt
# cat user.txt

!hdfs dfs -put user.txt prueba/
!hdfs dfs -ls prueba

"""- Mostrar su contenido"""

!hdfs dfs -cat prueba/user.txt

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# hdfs dfs -tail prueba/user.txt