# -*- coding: utf-8 -*-
"""Act_4_1_Segmentacion clientes bancarios.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R89bPf6mcyTA9A2p-ckfNsYhRNhRP9R5

***Adrián Yared Armas de la Nuez***

Input variables:
   # bank client data:
   1 - age (numeric)

   2 - job : type of job (categorical: "admin.","unknown","unemployed","management","housemaid","entrepreneur","student","blue-collar","self-employed","retired","technician","services")

   3 - marital : marital status (categorical: "married","divorced","single"; note: "divorced" means divorced or widowed)

   4 - education (categorical: "unknown","secondary","primary","tertiary")

   5 - default: has credit in default? (binary: "yes","no")

   6 - balance: average yearly balance, in euros (numeric)

   7 - housing: has housing loan? (binary: "yes","no")

   8 - loan: has personal loan? (binary: "yes","no")

   # related with the last contact of the current campaign:
   9 - contact: contact communication type (categorical: "unknown","telephone","cellular")

  10 - day: last contact day of the month (numeric)

  11 - month: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")

  12 - duration: last contact duration, in seconds (numeric)

   # other attributes:
  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)

  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)

  15 - previous: number of contacts performed before this campaign and for this client (numeric)

  16 - poutcome: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success")

  
  Output variable (desired target):
  
  17 - y - has the client subscribed a term deposit? (binary: "yes","no")
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
# Importación de los datos
#repositorio = 'https://raw.githubusercontent.com/SalvadorBR/SNS_2324_ACT_4_1/main/bank%2Bmarketing/bank/bank-full.csv'
repositorio = 'https://raw.githubusercontent.com/SalvadorBR/SNS_2324_ACT_4_1/main/bank%2Bmarketing/bank/bank.csv'


df_clients_ori = pd.read_csv(repositorio, sep=';')
columns =df_clients_ori.columns.values

df_clients_ori

columns

for columna in df_clients_ori.columns:
    num_valores_unicos = df_clients_ori[columna].nunique()
    print(f'Número de valores únicos en la columna {columna}: {num_valores_unicos}')

# Elimino las columnas que considero no son interesantes para realizar el clustering
# ['age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'contact','day', 'month', 'duration','campaign', 'pdays', 'previous', 'poutcome', 'y']
# No voy a continuar con las siguientes columnas: 'day', 'month', 'default', 'previous', 'pdays'
columns_selected = ['age', 'job', 'marital', 'education', 'balance', 'housing', 'loan', 'contact',  'duration','campaign', 'poutcome', 'y']

df_clients = df_clients_ori[columns_selected]
df_clients

# Conversión de variables catgóricas a numéricas
le = LabelEncoder()
pd.options.mode.copy_on_write = True # Para que no muestre el warning

df_clients['job'] = le.fit_transform(df_clients['job'])
df_clients['marital'] = le.fit_transform(df_clients['marital'])
df_clients['education'] = le.fit_transform(df_clients['education'])
#df_clients['default'] = le.fit_transform(df_clients['default'])
df_clients['housing'] = le.fit_transform(df_clients['housing'])
df_clients['loan'] = le.fit_transform(df_clients['loan'])
df_clients['contact'] = le.fit_transform(df_clients['contact'])
df_clients['poutcome'] = le.fit_transform(df_clients['poutcome'])
df_clients['y'] = le.fit_transform(df_clients['y'])

df_clients

"""## Transformación de los datos


"""

from sklearn import preprocessing

min_max_scaler = preprocessing.MinMaxScaler()
clients_scaled = min_max_scaler.fit_transform(df_clients)
clients_scaled

"""Tenemos que ser conscientes de que *clients_scaled* es ahora una matriz de numpy, que no es la misma *estructura de datos* que *df_clients* (que es un dataframe).

Antes de realizar el clustering sería recomendable seleccionar aquellas variables que nos parezcan más significativas para el problema.  Esto es recomendable incluso antes de realizar el procesamiento de los datos, pero esto sólo es posible si conocemos bien el problema de antemano.

Si no conocemos el problema, podemos estudiar cómo se distribuyen esos datos una vez han sido cargados, como por ejemplo, ver la correlación entre variables y/o realizar un **análisis de componentes principales (PCA)** , con el objetyivo de detectar si hay algún patrón que nos pueda ayudar a tomar una determinada estrategia, a continuación obtener una proyección 2D de los datos que podamos visualizar y poder así tomar una decisión al respecto.
"""

from sklearn.decomposition import PCA
import numpy as np

# Reducimos la dimensionalidad de los datos (a dos dimensiones)
pca = PCA(n_components = 2)
X_pca = pca.fit_transform(clients_scaled)
# Mostramos el porcentaje de varianza explicada por cada uno de los componentes seleccionados.
print(pca.explained_variance_ratio_)

# Visualizar la "importancia" de cada variable original del problema en las nuevas dimensiones
pd.DataFrame(np.matrix.transpose(pca.components_), columns=['PC-1', 'PC-2'], index=df_clients.columns)

# Visualizar el dataset utilizando las dos dimensiones obtenidas en el PCA
import matplotlib.pyplot as plt

plt.figure(figsize=(9, 6))
for i in range(len(X_pca)):
    plt.text(X_pca[i][0], X_pca[i][1], 'x', color="b")
plt.xlim(-1.1, 1.5)
plt.ylim(-2.5, 2.5)
plt.title("Datos de clientes tras aplicar PCA")
plt.grid()
plt.show()

"""## Ejecución del algoritmo de clustering jerárquico

A continuación, vamos a ejecutar un algoritmo de clustering jerárquico (notar que tardará un rato en ejecutarse) para ver cómo se distribuyen los datos.

Dado que la mayor parte de los datos no están muy concentrados y el resto parecen valores atípicos, es mejor utilizar como distancia intercluster el **vecino más próximo** (single). Las opciones más recomendadas para este problema van desde el **vecino más alejado** (complete) hasta la **mínima varianza** (ward).

Utilizaremos como distancia el vecino más cercano (single) ya que nos permitirá dividir los grupos más grandes. Se establece un umbral (threshold) de $15$ como distancia a partir de la cual se considerará que los grupos a distancia mayor son grupos independientes (y por lo tanto no se agruparan).

"""

from sklearn.metrics import pairwise_distances
import numpy as np
import matplotlib.pyplot as plt
from scipy import cluster

# 1. Calcular la matriz de distancias
D = pairwise_distances(clients_scaled, metric='euclidean')
avD = np.average(D)
print("Distancia Media\t {:6.2f}".format(avD))

# 2. Construir dendrograma
plt.figure(figsize=(12, 6))
clusters = cluster.hierarchy.linkage(D, method='single')  # Puedes cambiar a 'ward' o 'complete'
cluster.hierarchy.dendrogram(clusters, color_threshold=15)
plt.title("Dendograma con datos de clientes")
plt.show()

"""Tras realizar el corte a una distancia de $15$ utilizando distancia intercluster el vecino más alejado (complete), obtenemos $7$ grupos. Este nivel de corte debe realizarse ad-hoc en un nível que consideremos lógico. En este caso concreto, vemos que realizar este corte nos da unos cuantos outiliers (grupos de un solo elemento).

A continuación vamos a obtener un vector (*labels*) que, dada una distancia de corte (*threshold*) y un criterio de distancia intercluster, contenga el grupo al que pertece cada elemento.
"""

# 3. Obtenemos el grupo al que pertenece cada observación
threshold = 15 # ad-hoc
labels = cluster.hierarchy.fcluster(clusters, threshold , criterion='distance')

# ¿Cuántos grupos hay? Contamos el número de "labels" distintas en el vector
print("Número de clusters {}".format(len(set(labels))))
labels

"""Ahora podemos volver a representar gráficamente los datos del PCA (en dos dimensiones) pero introduciendo como color los resultados de las etiquetas. Se puede ver que el clustering no ha quedado nada mal, los resultados son coherentes."""

colores = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])
colores = np.hstack([colores] * 20)

plt.figure(figsize=(9, 6))
for i in range(len(X_pca)):
    plt.text(X_pca[i][0], X_pca[i][1], 'x', color=colores[labels[i]])
plt.xlim(-1.1, 1.5)
plt.ylim(-2.5, 2.5)
plt.grid()
plt.title("Datos Clientes tras aplicar PCA y  detectados los clusters")
plt.show()

"""## Calidad del clustering"""

from sklearn.metrics import silhouette_score
from sklearn import preprocessing
from scipy import cluster

# Normalización
scaler = preprocessing.MinMaxScaler()
clients_scaled = scaler.fit_transform(df_clients)

# Clustering jerárquico
linkage_matrix = cluster.hierarchy.linkage(clients_scaled, method='ward')
labels = cluster.hierarchy.fcluster(linkage_matrix, t=15, criterion='distance')

# Silhouette
silhouette = silhouette_score(clients_scaled, labels)
print(f"Puntuación de silueta: {silhouette:.2f}")

"""## Análisis de los clústers obtenidos"""

df_clients['Cluster'] = labels
print(df_clients.groupby('Cluster').mean())